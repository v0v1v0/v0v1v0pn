<div class="container">

<table style="width: 100%;"><tr>
<td>tokenizers</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Regexp tokenizers</h2>

<h3>Description</h3>

<p>Tokenizers using regular expressions to match either tokens or
separators between tokens.
</p>


<h3>Usage</h3>

<pre><code class="language-R">Regexp_Tokenizer(pattern, invert = FALSE, ..., meta = list())
blankline_tokenizer(s)
whitespace_tokenizer(s)
wordpunct_tokenizer(s)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>pattern</code></td>
<td>
<p>a character string giving the regular expression to use
for matching.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>invert</code></td>
<td>
<p>a logical indicating whether to match separators between
tokens.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>...</code></td>
<td>
<p>further arguments to be passed to <code>gregexpr()</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>meta</code></td>
<td>
<p>a named or empty list of tokenizer metadata tag-value
pairs.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>s</code></td>
<td>
<p>a <code>String</code> object, or something coercible to this
using <code>as.String()</code> (e.g., a character string with
appropriate encoding information).</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p><code>Regexp_Tokenizer()</code> creates regexp span tokenizers which use the
given <code>pattern</code> and <code>...</code> arguments to match tokens or
separators between tokens via <code>gregexpr()</code>, and then
transform the results of this into character spans of the tokens
found.
</p>
<p><code>whitespace_tokenizer()</code> tokenizes by treating any sequence of
whitespace characters as a separator.
</p>
<p><code>blankline_tokenizer()</code> tokenizes by treating any sequence of
blank lines as a separator.
</p>
<p><code>wordpunct_tokenizer()</code> tokenizes by matching sequences of
alphabetic characters and sequences of (non-whitespace) non-alphabetic
characters.
</p>


<h3>Value</h3>

<p><code>Regexp_Tokenizer()</code> returns the created regexp span tokenizer.
</p>
<p><code>blankline_tokenizer()</code>, <code>whitespace_tokenizer()</code> and
<code>wordpunct_tokenizer()</code> return the spans of the tokens found in
<code>s</code>.
</p>


<h3>See Also</h3>

<p><code>Span_Tokenizer()</code> for general information on span
tokenizer objects.
</p>


<h3>Examples</h3>

<pre><code class="language-R">## A simple text.
s &lt;- String("  First sentence.  Second sentence.  ")
##           ****5****0****5****0****5****0****5**

spans &lt;- whitespace_tokenizer(s)
spans
s[spans]

spans &lt;- wordpunct_tokenizer(s)
spans
s[spans]
</code></pre>


</div>