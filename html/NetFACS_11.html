<div class="container">

<table style="width: 100%;"><tr>
<td>entropy_overall</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Calculate information content of the dataset</h2>

<h3>Description</h3>

<p>Compares the observed and expected information content of the dataset.
</p>


<h3>Usage</h3>

<pre><code class="language-R">entropy_overall(x)
</code></pre>


<h3>Arguments</h3>

<table><tr style="vertical-align: top;">
<td><code>x</code></td>
<td>
<p>An object of class <code>netfacs</code> or simply a binary matrix
of 0s and 1s, with elements in columns and events in rows.</p>
</td>
</tr></table>
<h3>Value</h3>

<p>Function returns a summary <code>tibble</code>
containing the observed entropy, expected entropy and entropy ratio
(observed / expected) of the dataset. Observed entropy is calculated using
Shannon's information entropy formula <code class="reqn">- \sum_{i = 1}^n p_i \log
  (p_i)</code>. Expected entropy is based on randomization (shuffling the observed
elements while maintaining the number of elements per row) and represents
the maximum entropy that a dataset with the same properties as this one can
reach. Ratios closer to 0 are more ordered; ratios closer to 1 are more
random.
</p>


<h3>References</h3>

<p>Shannon, C. E. (1948). A Mathematical Theory of Communication.
<em>Bell System Technical Journal</em>.
<code>https://doi.org/10.1002/j.1538-7305.1948.tb01338.x</code>
</p>


<h3>Examples</h3>

<pre><code class="language-R">### how do angry facial expressions differ from non-angry ones?
data(emotions_set)
angry.face &lt;- netfacs(
  data = emotions_set[[1]],
  condition = emotions_set[[2]]$emotion,
  test.condition = "anger",
  ran.trials = 100,
  combination.size = 2
)

entropy_overall(angry.face)
</code></pre>


</div>