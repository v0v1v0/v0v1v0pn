<div class="container">

<table style="width: 100%;"><tr>
<td>kld</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Kullback-Leibler Divergence</h2>

<h3>Description</h3>

<p>Estimates the Kullback-Leibler Divergence which measures how one probability distribution
diverges from the original distribution (equivalent means are assumed)
Matrices <strong>must</strong> be positive definite inverse covariance matrix for accurate measurement.
This is a <strong>relative</strong> metric
</p>


<h3>Usage</h3>

<pre><code class="language-R">kld(base, test)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>base</code></td>
<td>
<p>Full or base model</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>test</code></td>
<td>
<p>Reduced or testing model</p>
</td>
</tr>
</table>
<h3>Value</h3>

<p>A value greater than 0.
Smaller values suggest the probability distribution of the reduced model is near the full model
</p>


<h3>Author(s)</h3>

<p>Alexander Christensen &lt;alexpaulchristensen@gmail.com&gt;
</p>


<h3>References</h3>

<p>Kullback, S., &amp; Leibler, R. A. (1951).
On information and sufficiency.
<em>The Annals of Mathematical Statistics</em>, <em>22</em>, 79-86.
</p>


<h3>Examples</h3>

<pre><code class="language-R">A1 &lt;- solve(cov(neoOpen))

## Not run: 
A2 &lt;- LoGo(neoOpen)

kld_value &lt;- kld(A1, A2)

## End(Not run)

</code></pre>


</div>