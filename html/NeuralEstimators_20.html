<div class="container">

<table style="width: 100%;"><tr>
<td>train</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Train a neural estimator</h2>

<h3>Description</h3>

<p>The function caters for different variants of "on-the-fly" simulation.
Specifically, a <code>sampler</code> can be provided to continuously sample new
parameter vectors from the prior, and a <code>simulator</code> can be provided to
continuously simulate new data conditional on the parameters. If provided
with specific sets of parameters (<code>theta_train</code> and <code>theta_val</code>)
and/or data (<code>Z_train</code> and <code>Z_val</code>), they will be held fixed during
training.
</p>
<p>Note that using <code>R</code> functions to perform "on-the-fly" simulation requires the user to have installed the Julia package <code>RCall</code>.
</p>


<h3>Usage</h3>

<pre><code class="language-R">train(
  estimator,
  sampler = NULL,
  simulator = NULL,
  theta_train = NULL,
  theta_val = NULL,
  Z_train = NULL,
  Z_val = NULL,
  m = NULL,
  M = NULL,
  K = 10000,
  xi = NULL,
  loss = "absolute-error",
  learning_rate = 1e-04,
  epochs = 100,
  batchsize = 32,
  savepath = "",
  stopping_epochs = 5,
  epochs_per_Z_refresh = 1,
  epochs_per_theta_refresh = 1,
  simulate_just_in_time = FALSE,
  use_gpu = TRUE,
  verbose = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>estimator</code></td>
<td>
<p>a neural estimator</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>sampler</code></td>
<td>
<p>a function that takes an integer <code>K</code>, samples <code>K</code> parameter vectors from the prior, and returns them as a px<code>K</code> matrix</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>simulator</code></td>
<td>
<p>a function that takes a px<code>K</code> matrix of parameters and an integer <code>m</code>, and returns <code>K</code> simulated data sets each containing <code>m</code> independent replicates</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>theta_train</code></td>
<td>
<p>a set of parameters used for updating the estimator using stochastic gradient descent</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>theta_val</code></td>
<td>
<p>a set of parameters used for monitoring the performance of the estimator during training</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>Z_train</code></td>
<td>
<p>a simulated data set used for updating the estimator using stochastic gradient descent</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>Z_val</code></td>
<td>
<p>a simulated data set used for monitoring the performance of the estimator during training</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>m</code></td>
<td>
<p>vector of sample sizes. If <code>NULL</code> (default), a single neural estimator is trained, with the sample size inferred from <code>Z_val</code>. If <code>m</code> is a vector of integers, a sequence of neural estimators is constructed for each sample size; see the Julia documentation for <code>trainx()</code> for further details</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>M</code></td>
<td>
<p>deprecated; use <code>m</code></p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>K</code></td>
<td>
<p>the number of parameter vectors sampled in the training set at each epoch; the size of the validation set is set to <code>K</code>/5.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>xi</code></td>
<td>
<p>a list of objects used for data simulation (e.g., distance matrices); if it is provided, the parameter sampler is called as <code>sampler(K, xi)</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>loss</code></td>
<td>
<p>the loss function: a string ('absolute-error' for mean-absolute-error loss or 'squared-error' for mean-squared-error loss), or a string of Julia code defining the loss function.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>learning_rate</code></td>
<td>
<p>the learning rate for the optimiser ADAM (default 1e-3)</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>epochs</code></td>
<td>
<p>the number of epochs</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>batchsize</code></td>
<td>
<p>the batchsize to use when performing stochastic gradient descent</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>savepath</code></td>
<td>
<p>path to save the trained estimator and other information; if null (default), nothing is saved. Otherwise, the neural-network parameters (i.e., the weights and biases) will be saved during training as <code>bson</code> files; the risk function evaluated over the training and validation sets will also be saved, in the first and second columns of <code>loss_per_epoch.csv</code>, respectively; the best parameters (as measured by validation risk) will be saved as <code>best_network.bson</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>stopping_epochs</code></td>
<td>
<p>cease training if the risk doesn't improve in this number of epochs (default 5)</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>epochs_per_Z_refresh</code></td>
<td>
<p>integer indicating how often to refresh the training data</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>epochs_per_theta_refresh</code></td>
<td>
<p>integer indicating how often to refresh the training parameters; must be a multiple of <code>epochs_per_Z_refresh</code></p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>simulate_just_in_time</code></td>
<td>
<p>flag indicating whether we should simulate "just-in-time", in the sense that only a <code>batchsize</code> number of parameter vectors and corresponding data are in memory at a given time</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>use_gpu</code></td>
<td>
<p>a boolean indicating whether to use the GPU if it is available (default true)</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>verbose</code></td>
<td>
<p>a boolean indicating whether information should be printed to the console during training</p>
</td>
</tr>
</table>
<h3>Value</h3>

<p>a trained neural estimator or, if <code>m</code> is a vector, a list of trained neural estimators
</p>


<h3>See Also</h3>

<p><code>assess()</code> for assessing an estimator post training, and <code>estimate()</code> for applying an estimator to observed data
</p>


<h3>Examples</h3>

<pre><code class="language-R">## Not run: 
# Construct a neural Bayes estimator for replicated univariate Gaussian 
# data with unknown mean and standard deviation. 

# Load R and Julia packages
library("NeuralEstimators")
library("JuliaConnectoR")
juliaEval("using NeuralEstimators, Flux, Distributions")

# Define the neural-network architecture
estimator &lt;- juliaEval('
 d = 1    # dimension of each replicate
 p = 2    # number of parameters in the model
 w = 32   # width of each layer
 psi = Chain(Dense(d, w, relu), Dense(w, w, relu))
 phi = Chain(Dense(w, w, relu), Dense(w, p))
 deepset = DeepSet(psi, phi)
 estimator = PointEstimator(deepset)
')

# Sampler from the prior
sampler &lt;- function(K) {
  mu    &lt;- rnorm(K)      # Gaussian prior for the mean
  sigma &lt;- rgamma(K, 1)  # Gamma prior for the standard deviation
  theta &lt;- matrix(c(mu, sigma), byrow = TRUE, ncol = K)
  return(theta)
}

# Data simulator
simulator &lt;- function(theta_set, m) {
  apply(theta_set, 2, function(theta) {
    t(rnorm(m, theta[1], theta[2]))
  }, simplify = FALSE)
}

# Train using fixed parameter and data sets 
theta_train &lt;- sampler(10000)
theta_val   &lt;- sampler(2000)
m &lt;- 30 # number of iid replicates
Z_train &lt;- simulator(theta_train, m)
Z_val   &lt;- simulator(theta_val, m)
estimator &lt;- train(estimator, 
                   theta_train = theta_train, 
                   theta_val = theta_val, 
                   Z_train = Z_train, 
                   Z_val = Z_val)
                   
# Train using simulation on-the-fly (requires Julia package RCall)
estimator &lt;- train(estimator, sampler = sampler, simulator = simulator, m = m)

##### Simulation on-the-fly using Julia functions ####

# Defining the sampler and simulator in Julia can improve computational 
# efficiency by avoiding the overhead of communicating between R and Julia. 
# Julia is also fast (comparable to C) and so it can be useful to define 
# these functions in Julia when they involve for-loops. 

# Parameter sampler
sampler &lt;- juliaEval("
      function sampler(K)
      	mu = rand(Normal(0, 1), K)
      	sigma = rand(Gamma(1), K)
      	theta = hcat(mu, sigma)'
      	return theta
      end")

# Data simulator
simulator &lt;- juliaEval("
      function simulator(theta_matrix, m)
      	Z = [rand(Normal(theta[1], theta[2]), 1, m) for theta in eachcol(theta_matrix)]
      	return Z
      end")

# Train the estimator
estimator &lt;- train(estimator, sampler = sampler, simulator = simulator, m = m)
## End(Not run)
</code></pre>


</div>