<div class="container">

<table style="width: 100%;"><tr>
<td>HessianMLP</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Sensitivity of MLP models</h2>

<h3>Description</h3>

<p>Function for evaluating the sensitivities of the inputs
variables in a mlp model
</p>


<h3>Usage</h3>

<pre><code class="language-R">HessianMLP(
  MLP.fit,
  .returnSens = TRUE,
  plot = TRUE,
  .rawSens = FALSE,
  sens_origin_layer = 1,
  sens_end_layer = "last",
  sens_origin_input = TRUE,
  sens_end_input = FALSE,
  ...
)

## Default S3 method:
HessianMLP(
  MLP.fit,
  .returnSens = TRUE,
  plot = TRUE,
  .rawSens = FALSE,
  sens_origin_layer = 1,
  sens_end_layer = "last",
  sens_origin_input = TRUE,
  sens_end_input = FALSE,
  trData,
  actfunc = NULL,
  deractfunc = NULL,
  der2actfunc = NULL,
  preProc = NULL,
  terms = NULL,
  output_name = NULL,
  ...
)

## S3 method for class 'train'
HessianMLP(
  MLP.fit,
  .returnSens = TRUE,
  plot = TRUE,
  .rawSens = FALSE,
  sens_origin_layer = 1,
  sens_end_layer = "last",
  sens_origin_input = TRUE,
  sens_end_input = FALSE,
  ...
)

## S3 method for class 'H2OMultinomialModel'
HessianMLP(
  MLP.fit,
  .returnSens = TRUE,
  plot = TRUE,
  .rawSens = FALSE,
  sens_origin_layer = 1,
  sens_end_layer = "last",
  sens_origin_input = TRUE,
  sens_end_input = FALSE,
  ...
)

## S3 method for class 'H2ORegressionModel'
HessianMLP(
  MLP.fit,
  .returnSens = TRUE,
  plot = TRUE,
  .rawSens = FALSE,
  sens_origin_layer = 1,
  sens_end_layer = "last",
  sens_origin_input = TRUE,
  sens_end_input = FALSE,
  ...
)

## S3 method for class 'list'
HessianMLP(
  MLP.fit,
  .returnSens = TRUE,
  plot = TRUE,
  .rawSens = FALSE,
  sens_origin_layer = 1,
  sens_end_layer = "last",
  sens_origin_input = TRUE,
  sens_end_input = FALSE,
  trData,
  actfunc,
  ...
)

## S3 method for class 'mlp'
HessianMLP(
  MLP.fit,
  .returnSens = TRUE,
  plot = TRUE,
  .rawSens = FALSE,
  sens_origin_layer = 1,
  sens_end_layer = "last",
  sens_origin_input = TRUE,
  sens_end_input = FALSE,
  trData,
  preProc = NULL,
  terms = NULL,
  ...
)

## S3 method for class 'nn'
HessianMLP(
  MLP.fit,
  .returnSens = TRUE,
  plot = TRUE,
  .rawSens = FALSE,
  sens_origin_layer = 1,
  sens_end_layer = "last",
  sens_origin_input = TRUE,
  sens_end_input = FALSE,
  preProc = NULL,
  terms = NULL,
  ...
)

## S3 method for class 'nnet'
HessianMLP(
  MLP.fit,
  .returnSens = TRUE,
  plot = TRUE,
  .rawSens = FALSE,
  sens_origin_layer = 1,
  sens_end_layer = "last",
  sens_origin_input = TRUE,
  sens_end_input = FALSE,
  trData,
  preProc = NULL,
  terms = NULL,
  ...
)

## S3 method for class 'nnetar'
HessianMLP(
  MLP.fit,
  .returnSens = TRUE,
  plot = TRUE,
  .rawSens = FALSE,
  sens_origin_layer = 1,
  sens_end_layer = "last",
  sens_origin_input = TRUE,
  sens_end_input = FALSE,
  ...
)

## S3 method for class 'numeric'
HessianMLP(
  MLP.fit,
  .returnSens = TRUE,
  plot = TRUE,
  .rawSens = FALSE,
  sens_origin_layer = 1,
  sens_end_layer = "last",
  sens_origin_input = TRUE,
  sens_end_input = FALSE,
  trData,
  actfunc = NULL,
  preProc = NULL,
  terms = NULL,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>MLP.fit</code></td>
<td>
<p>fitted neural network model</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>.returnSens</code></td>
<td>
<p>DEPRECATED</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>plot</code></td>
<td>
<p><code>logical</code> whether or not to plot the analysis. By default is
<code>TRUE</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>.rawSens</code></td>
<td>
<p>DEPRECATED</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>sens_origin_layer</code></td>
<td>
<p><code>numeric</code> specifies the layer of neurons with
respect to which the derivative must be calculated. The input layer is
specified by 1 (default).</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>sens_end_layer</code></td>
<td>
<p><code>numeric</code> specifies the layer of neurons of which
the derivative is calculated. It may also be 'last' to specify the output
layer (default).</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>sens_origin_input</code></td>
<td>
<p><code>logical</code> specifies if the derivative must be
calculated with respect to the inputs (<code>TRUE</code>) or output
(<code>FALSE</code>) of the <code>sens_origin_layer</code> layer of the model. By
default is <code>TRUE</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>sens_end_input</code></td>
<td>
<p><code>logical</code> specifies if the derivative calculated
is of the output (<code>FALSE</code>) or from the input (<code>TRUE</code>) of the
<code>sens_end_layer</code> layer of the model. By default is <code>FALSE</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>...</code></td>
<td>
<p>additional arguments passed to or from other methods</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>trData</code></td>
<td>
<p><code>data.frame</code> containing the data to evaluate the sensitivity of the model</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>actfunc</code></td>
<td>
<p><code>character</code> vector indicating the activation function of each
neurons layer.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>deractfunc</code></td>
<td>
<p><code>character</code> vector indicating the derivative of the activation
function of each neurons layer.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>der2actfunc</code></td>
<td>
<p><code>character</code> vector indicating the second derivative of the activation
function of each neurons layer.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>preProc</code></td>
<td>
<p>preProcess structure applied to the training data. See also
<code>preProcess</code></p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>terms</code></td>
<td>
<p>function applied to the training data to create factors. See
also <code>train</code></p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>output_name</code></td>
<td>
<p><code>character</code> name of the output variable in order to
avoid changing the name of the output variable in <code>trData</code> to
'.outcome'</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>In case of using an input of class <code>factor</code> and a package which
need to enter the input data as matrix, the dummies must be created before
training the neural network.
</p>
<p>After that, the training data must be given to the function using the
<code>trData</code> argument.
</p>


<h3>Value</h3>

<p><code>SensMLP</code> object with the sensitivity metrics and sensitivities of
the MLP model passed to the function.
</p>


<h3>Plots</h3>

 <ul>
<li>
<p> Plot 1: colorful plot with the classification
of the classes in a 2D map </p>
</li>
<li>
<p> Plot 2: b/w plot with probability of the
chosen class in a 2D map </p>
</li>
<li>
<p> Plot 3: plot with the stats::predictions of
the data provided </p>
</li>
</ul>
<h3>Examples</h3>

<pre><code class="language-R">## Load data -------------------------------------------------------------------
data("DAILY_DEMAND_TR")
fdata &lt;- DAILY_DEMAND_TR
## Parameters of the NNET ------------------------------------------------------
hidden_neurons &lt;- 5
iters &lt;- 100
decay &lt;- 0.1

################################################################################
#########################  REGRESSION NNET #####################################
################################################################################
## Regression dataframe --------------------------------------------------------
# Scale the data
fdata.Reg.tr &lt;- fdata[,2:ncol(fdata)]
fdata.Reg.tr[,3] &lt;- fdata.Reg.tr[,3]/10
fdata.Reg.tr[,1] &lt;- fdata.Reg.tr[,1]/1000

# Normalize the data for some models
preProc &lt;- caret::preProcess(fdata.Reg.tr, method = c("center","scale"))
nntrData &lt;- predict(preProc, fdata.Reg.tr)

#' ## TRAIN nnet NNET --------------------------------------------------------
# Create a formula to train NNET
form &lt;- paste(names(fdata.Reg.tr)[2:ncol(fdata.Reg.tr)], collapse = " + ")
form &lt;- formula(paste(names(fdata.Reg.tr)[1], form, sep = " ~ "))

set.seed(150)
nnetmod &lt;- nnet::nnet(form,
                      data = nntrData,
                      linear.output = TRUE,
                      size = hidden_neurons,
                      decay = decay,
                      maxit = iters)
# Try HessianMLP
NeuralSens::HessianMLP(nnetmod, trData = nntrData, plot = FALSE)

# Try HessianMLP to calculate sensitivities with respect to output of hidden neurones
NeuralSens::HessianMLP(nnetmod, trData = nntrData,
                             sens_origin_layer = 2,
                             sens_end_layer = "last",
                             sens_origin_input = FALSE,
                             sens_end_input = FALSE)
## Train caret NNET ------------------------------------------------------------
# Create trainControl
ctrl_tune &lt;- caret::trainControl(method = "boot",
                                 savePredictions = FALSE,
                                 summaryFunction = caret::defaultSummary)
set.seed(150) #For replication
caretmod &lt;- caret::train(form = DEM~.,
                              data = fdata.Reg.tr,
                              method = "nnet",
                              linout = TRUE,
                              tuneGrid = data.frame(size = 3,
                                                    decay = decay),
                              maxit = iters,
                              preProcess = c("center","scale"),
                              trControl = ctrl_tune,
                              metric = "RMSE")

# Try HessianMLP
NeuralSens::HessianMLP(caretmod)

## Train h2o NNET --------------------------------------------------------------
# Create a cluster with 4 available cores
h2o::h2o.init(ip = "localhost",
              nthreads = 4)

# Reset the cluster
h2o::h2o.removeAll()
fdata_h2o &lt;- h2o::as.h2o(x = fdata.Reg.tr, destination_frame = "fdata_h2o")

set.seed(150)
h2omod &lt;-h2o:: h2o.deeplearning(x = names(fdata.Reg.tr)[2:ncol(fdata.Reg.tr)],
                                     y = names(fdata.Reg.tr)[1],
                                     distribution = "AUTO",
                                     training_frame = fdata_h2o,
                                     standardize = TRUE,
                                     activation = "Tanh",
                                     hidden = c(hidden_neurons),
                                     stopping_rounds = 0,
                                     epochs = iters,
                                     seed = 150,
                                     model_id = "nnet_h2o",
                                     adaptive_rate = FALSE,
                                     rate_decay = decay,
                                     export_weights_and_biases = TRUE)

# Try HessianMLP
NeuralSens::HessianMLP(h2omod)

# Turn off the cluster
h2o::h2o.shutdown(prompt = FALSE)
rm(fdata_h2o)

## Train RSNNS NNET ------------------------------------------------------------
# Normalize data using RSNNS algorithms
trData &lt;- as.data.frame(RSNNS::normalizeData(fdata.Reg.tr))
names(trData) &lt;- names(fdata.Reg.tr)
set.seed(150)
RSNNSmod &lt;-RSNNS::mlp(x = trData[,2:ncol(trData)],
                           y = trData[,1],
                           size = hidden_neurons,
                           linOut = TRUE,
                           learnFuncParams=c(decay),
                           maxit=iters)

# Try HessianMLP
NeuralSens::HessianMLP(RSNNSmod, trData = trData, output_name = "DEM")

## USE DEFAULT METHOD ----------------------------------------------------------
NeuralSens::HessianMLP(caretmod$finalModel$wts,
                            trData = fdata.Reg.tr,
                            mlpstr = caretmod$finalModel$n,
                            coefnames = caretmod$coefnames,
                            actfun = c("linear","sigmoid","linear"),
                            output_name = "DEM")

################################################################################
#########################  CLASSIFICATION NNET #################################
################################################################################
## Regression dataframe --------------------------------------------------------
# Scale the data
fdata.Reg.cl &lt;- fdata[,2:ncol(fdata)]
fdata.Reg.cl[,2:3] &lt;- fdata.Reg.cl[,2:3]/10
fdata.Reg.cl[,1] &lt;- fdata.Reg.cl[,1]/1000

# Normalize the data for some models
preProc &lt;- caret::preProcess(fdata.Reg.cl, method = c("center","scale"))
nntrData &lt;- predict(preProc, fdata.Reg.cl)

# Factorize the output
fdata.Reg.cl$DEM &lt;- factor(round(fdata.Reg.cl$DEM, digits = 1))

# Normalize the data for some models
preProc &lt;- caret::preProcess(fdata.Reg.cl, method = c("center","scale"))
nntrData &lt;- predict(preProc, fdata.Reg.cl)

## Train caret NNET ------------------------------------------------------------
# Create trainControl
ctrl_tune &lt;- caret::trainControl(method = "boot",
                                 savePredictions = FALSE,
                                 summaryFunction = caret::defaultSummary)
set.seed(150) #For replication
caretmod &lt;- caret::train(form = DEM~.,
                                data = fdata.Reg.cl,
                                method = "nnet",
                                linout = FALSE,
                                tuneGrid = data.frame(size = hidden_neurons,
                                                      decay = decay),
                                maxit = iters,
                                preProcess = c("center","scale"),
                                trControl = ctrl_tune,
                                metric = "Accuracy")

# Try HessianMLP
NeuralSens::HessianMLP(caretmod)

## Train h2o NNET --------------------------------------------------------------
# Create local cluster with 4 available cores
h2o::h2o.init(ip = "localhost",
              nthreads = 4)

# Reset the cluster
h2o::h2o.removeAll()
fdata_h2o &lt;- h2o::as.h2o(x = fdata.Reg.cl, destination_frame = "fdata_h2o")

set.seed(150)
h2omod &lt;- h2o::h2o.deeplearning(x = names(fdata.Reg.cl)[2:ncol(fdata.Reg.cl)],
                                       y = names(fdata.Reg.cl)[1],
                                       distribution = "AUTO",
                                       training_frame = fdata_h2o,
                                       standardize = TRUE,
                                       activation = "Tanh",
                                       hidden = c(hidden_neurons),
                                       stopping_rounds = 0,
                                       epochs = iters,
                                       seed = 150,
                                       model_id = "nnet_h2o",
                                       adaptive_rate = FALSE,
                                       rate_decay = decay,
                                       export_weights_and_biases = TRUE)

# Try HessianMLP
NeuralSens::HessianMLP(h2omod)

# Apaga el cluster
h2o::h2o.shutdown(prompt = FALSE)
rm(fdata_h2o)

## TRAIN nnet NNET --------------------------------------------------------
# Create a formula to train NNET
form &lt;- paste(names(fdata.Reg.tr)[2:ncol(fdata.Reg.tr)], collapse = " + ")
form &lt;- formula(paste(names(fdata.Reg.tr)[1], form, sep = " ~ "))

set.seed(150)
nnetmod &lt;- nnet::nnet(form,
                      data = nntrData,
                      linear.output = TRUE,
                      size = hidden_neurons,
                      decay = decay,
                      maxit = iters)
# Try HessianMLP
NeuralSens::HessianMLP(nnetmod, trData = nntrData)

</code></pre>


</div>