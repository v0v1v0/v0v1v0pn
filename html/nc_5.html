<div class="container">

<table style="width: 100%;"><tr>
<td>capture_all_str</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Capture all matches in a single subject string</h2>

<h3>Description</h3>

<p>Capture each match of a regex pattern from one multi-line subject
string or text file. It can be used to convert any regular text
file (web page, log, etc) to a data table, see examples.</p>


<h3>Usage</h3>

<pre><code class="language-R">capture_all_str(..., 
    engine = getOption("nc.engine", 
        "PCRE"), collapse = "\n")</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>...</code></td>
<td>
<p>subject, name1=pattern1, fun1, etc. The first argument must be a
subject character vector (or file name which is read via
<code>readLines</code> to get a subject). After removing missing values,
we use <code>paste</code> to <code>collapse</code> the subject (by default using
newline) and treat it as single character string to
search. Arguments after the first specify the regex/conversion and
must be string/function/list, as documented in <code>capture_first_vec</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>engine</code></td>
<td>
<p>character string, one of PCRE, ICU, RE2</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>collapse</code></td>
<td>
<p>separator string for combining elements of subject into a single
string, used as <code>collapse</code> argument of <code>paste</code>.</p>
</td>
</tr>
</table>
<h3>Value</h3>

<p>data.table with one row for each match, and one column for each
capture <code>group</code>.</p>


<h3>Author(s)</h3>

<p>Toby Hocking &lt;toby.hocking@r-project.org&gt; [aut, cre]</p>


<h3>Examples</h3>

<pre><code class="language-R">
data.table::setDTthreads(1)

chr.pos.vec &lt;- c(
  "chr10:213,054,000-213,055,000",
  "chrM:111,000-222,000",
  "this will not match",
  NA, # neither will this.
  "chr1:110-111 chr2:220-222") # two possible matches.
keep.digits &lt;- function(x)as.integer(gsub("[^0-9]", "", x))
## By default elements of subject are treated as separate lines (and
## NAs are removed). Named arguments are used to create capture
## groups, and conversion functions such as keep.digits are used to
## convert the previously named group.
int.pattern &lt;- list("[0-9,]+", keep.digits)
(match.dt &lt;- nc::capture_all_str(
  chr.pos.vec,
  chrom="chr.*?",
  ":",
  chromStart=int.pattern,
  "-",
  chromEnd=int.pattern))
str(match.dt)

## Extract all fields from each alignment block, using two regex
## patterns, then dcast.
info.txt.gz &lt;- system.file(
  "extdata", "SweeD_Info.txt.gz", package="nc")
info.vec &lt;- readLines(info.txt.gz)
info.vec[24:40]
info.dt &lt;- nc::capture_all_str(
  sub("Alignment ", "//", info.vec),
  "//",
  alignment="[0-9]+",
  fields="[^/]+")
(fields.dt &lt;- info.dt[, nc::capture_all_str(
  fields,
  "\t+",
  variable="[^:]+",
  ":\t*",
  value=".*"),
  by=alignment])
(fields.wide &lt;- data.table::dcast(fields.dt, alignment ~ variable))

## Capture all csv tables in report -- the file name can be given as
## the subject to nc::capture_all_str, which calls readLines to get
## data to parse.
(report.txt.gz &lt;- system.file(
  "extdata", "SweeD_Report.txt.gz", package="nc"))
(report.dt &lt;- nc::capture_all_str(
  report.txt.gz,
  "//",
  alignment="[0-9]+",
  "\n",
  csv="[^/]+"
)[, {
  data.table::fread(text=csv)
}, by=alignment])

## Join report with info fields.
report.dt[fields.wide, on=.(alignment)]

## parsing nbib citation file.
(pmc.nbib &lt;- system.file(
  "extdata", "PMC3045577.nbib", package="nc"))
blank &lt;- "\n      "
pmc.dt &lt;- nc::capture_all_str(
  pmc.nbib,
  Abbreviation="[A-Z]+",
  " *- ",
  value=list(
    ".*",
    list(blank, ".*"), "*"),
  function(x)sub(blank, "", x))
str(pmc.dt)

## What do the variable fields mean? It is explained on
## https://www.nlm.nih.gov/bsd/mms/medlineelements.html which has a
## local copy in this package (downloaded 18 Sep 2019).
fields.html &lt;- system.file(
  "extdata", "MEDLINE_Fields.html", package="nc")
if(interactive())browseURL(fields.html)
fields.vec &lt;- readLines(fields.html)

## It is pretty easy to capture fields and abbreviations if gsub
## used to remove some tags first.
no.strong &lt;- gsub("&lt;/?strong&gt;", "", fields.vec)
no.comments &lt;- gsub("&lt;!--.*?--&gt;", "", no.strong)
## grep then capture_first_vec can be used if each desired row in
## the output comes from a single line of the input file.
(h3.vec &lt;- grep("&lt;h3", no.comments, value=TRUE))
h3.pattern &lt;- list(
  nc::field("name", '="', '[^"]+'),
  '"&gt;&lt;/a&gt;',
  fields.abbrevs="[^&lt;]+")
first.fields.dt &lt;- nc::capture_first_vec(
  h3.vec, h3.pattern)
field.abbrev.pattern &lt;- list(
  Field=".*?",
  " \\(",
  Abbreviation="[^)]+",
  "\\)",
  "(?: and |$)?")
(first.each.field &lt;- first.fields.dt[, nc::capture_all_str(
  fields.abbrevs, field.abbrev.pattern),
  by=fields.abbrevs])

## If we want to capture the information after the initial h3 line
## of the input, e.g. the rest column below which contains a
## description/example for each field, then capture_all_str can be
## used on the full input file.
h3.fields.dt &lt;- nc::capture_all_str(
  no.comments,
  h3.pattern,
  '&lt;/h3&gt;\n',
  rest="(?:.*\n)+?", #exercise: get the examples.
  "&lt;hr /&gt;\n")
(h3.each.field &lt;- h3.fields.dt[, nc::capture_all_str(
  fields.abbrevs, field.abbrev.pattern),
  by=fields.abbrevs])

## Either method of capturing abbreviations gives the same result.
identical(first.each.field, h3.each.field)

## but the capture_all_str method returns the additional rest column
## which contains data after the initial h3 line.
names(first.fields.dt)
names(h3.fields.dt)
cat(h3.fields.dt[fields.abbrevs=="Volume (VI)", rest])

## There are 66 Field rows across three tables.
a.href &lt;- list('&lt;a href=[^&gt;]+&gt;')
(td.vec &lt;- fields.vec[240:280])
fields.pattern &lt;- list(
  "&lt;td.*?&gt;",
  a.href,
  Fields="[^()&lt;]+",
  "&lt;/a&gt;&lt;/td&gt;\n")
(td.only.Fields &lt;- nc::capture_all_str(fields.vec, fields.pattern))

## Extract Fields and Abbreviations. Careful: most fields have one
## abbreviation, but one field has none, and two fields have two.
(td.fields.dt &lt;- nc::capture_all_str(
  fields.vec,
  fields.pattern,
  "&lt;td[^&gt;]*&gt;",
  "(?:\n&lt;div&gt;)?",
  a.href, "?",
  abbrevs=".*?",
  "&lt;/"))

## Get each individual abbreviation from the previously captured td
## data.
td.each.field &lt;- td.fields.dt[, {
  f &lt;- nc::capture_all_str(
    Fields,
    Field=".*?",
    "(?:$| and )")
  a &lt;- nc::capture_all_str(
    abbrevs,
    "\\(",
    Abbreviation="[^)]+",
    "\\)")
  if(nrow(a)==0)list() else cbind(f, a)
}, by=Fields]
str(td.each.field)
td.each.field[td.fields.dt, .(
  count=.N
), on=.(Fields), by=.EACHI][order(count)]

## There is a typo in the data captured from the h3 headings.
td.each.field[!Field %in% h3.each.field$Field]
h3.each.field[!Field %in% td.each.field$Field]

## Abbreviations are consistent.
td.each.field[!Abbreviation %in% h3.each.field$Abbreviation]
h3.each.field[!Abbreviation %in% td.each.field$Abbreviation]

## There is a a table that provides a description of each comment
## type.
(comment.vec &lt;- fields.vec[840:860])
comment.dt &lt;- nc::capture_all_str(
  fields.vec,
  "&lt;td&gt;&lt;strong&gt;",
  Field="[^&lt;]+",
  "&lt;/strong&gt;&lt;/td&gt;\n",
  "&lt;td&gt;&lt;strong&gt;\\(",
  Abbreviation="[^)]+",
  "\\)&lt;/strong&gt;&lt;/td&gt;\n",
  "&lt;td&gt;",
  description=".*",
  "&lt;/td&gt;\n")
str(comment.dt)

## Join to original PMC citation file in order to see what the
## abbreviations used in that file mean.
all.abbrevs &lt;- rbind(
  td.each.field[, .(Field, Abbreviation)],
  comment.dt[, .(Field, Abbreviation)])
all.abbrevs[pmc.dt, .(
  Abbreviation,
  Field,
  value=substr(value, 1, 20)
), on=.(Abbreviation)]

## There is a listing of examples for each comment type.
(comment.ex.dt &lt;- nc::capture_all_str(
  fields.vec[938],
  "br /&gt;\\s*",
  Abbreviation="[A-Z]+",
  "\\s*-\\s*",
  citation="[^&lt;]+?",
  list(
    "[.] ",
    nc::field("PMID", ": ", "[0-9]+")
  ), "?",
  "&lt;"))

## Join abbreviations to see what kind of comments.
all.abbrevs[comment.ex.dt, on=.(Abbreviation)]

## parsing bibtex file.
refs.bib &lt;- system.file(
  "extdata", "namedCapture-refs.bib", package="nc")
refs.vec &lt;- readLines(refs.bib)
at.lines &lt;- grep("@", refs.vec, value=TRUE)
str(at.lines)
refs.dt &lt;- nc::capture_all_str(
  refs.vec,
  "@",
  type="[^{]+",
  "[{]",
  ref="[^,]+",
  ",\n",
  fields="(?:.*\n)+?.*",
  "[}]\\s*(?:$|\n)")
str(refs.dt)

## parsing each field of each entry.
eq.lines &lt;- grep("=", refs.vec, value=TRUE)
str(eq.lines)
strip &lt;- function(x)sub("^\\s*\\{*", "", sub("\\}*,?$", "", x))
refs.fields &lt;- refs.dt[, nc::capture_all_str(
  fields,
  "\\s+",
  variable="\\S+",
  "\\s+=",
  value=".*", strip),
  by=.(type, ref)]
str(refs.fields)
with(refs.fields[ref=="HockingUseR2011"], structure(
  as.list(value), names=variable))
## the URL of my talk is now
## https://user2011.r-project.org/TalkSlides/Lightening/2-StatisticsAndProg_3-Hocking.pdf

if(!grepl("solaris", R.version$platform)){#To avoid CRAN check error on solaris
  ## Parsing wikimedia tables: each begins with {| and ends with |}.
  emoji.txt.gz &lt;- system.file(
    "extdata", "wikipedia-emoji-text.txt.gz", package="nc")
  tables &lt;- nc::capture_all_str(
    emoji.txt.gz,
    "\n[{][|]",
    first=".*",
    '\n[|][+] style="',
    nc::field("font-size", ":", '.*?'),
    '" [|] ',
    title=".*",
    lines="(?:\n.*)*?",
    "\n[|][}]")
  str(tables)
  ## Rows are separated by |-
  rows.dt &lt;- tables[, {
    row.vec &lt;- strsplit(lines, "|-", fixed=TRUE)[[1]][-1]
    .(row.i=seq_along(row.vec), row=row.vec)
  }, by=title]
  str(rows.dt)
  ## Try to parse columns from each row. Doesn't work for second table
  ## https://en.wikipedia.org/w/index.php?title=Emoji&amp;oldid=920745513#Skin_color
  ## because some entries have rowspan=2.
  contents.dt &lt;- rows.dt[, nc::capture_all_str(
    row,
    "[|] ",
    content=".*?",
    "(?: [|]|\n|$)"),
    by=.(title, row.i)]
  contents.dt[, .(cols=.N), by=.(title, row.i)]
  ## Make data table from
  ## https://en.wikipedia.org/w/index.php?title=Emoji&amp;oldid=920745513#Emoji_versus_text_presentation
  contents.dt[, col.i := 1:.N, by=.(title, row.i)]
  data.table::dcast(
    contents.dt[title=="Sample emoji variation sequences"],
    row.i ~ col.i,
    value.var="content")
}

## Simple way to extract code chunks from Rmd.
vignette.Rmd &lt;- system.file(
  "extdata", "vignette.Rmd", package="nc")
non.greedy.lines &lt;- list(
  list(".*\n"), "*?")
optional.name &lt;- list(
  list(" ", name="[^,}]+"), "?")
Rmd.dt &lt;- nc::capture_all_str(
  vignette.Rmd,
  before=non.greedy.lines,
  "```\\{r",
  optional.name,
  parameters=".*",
  "\\}\n",
  code=non.greedy.lines,
  "```")
Rmd.dt[, chunk := 1:.N]
Rmd.dt[, .(chunk, name, parameters, some.code=substr(code, 1, 20))]

## Extract individual parameter names and values.
Rmd.dt[, nc::capture_all_str(
  parameters,
  ", *",
  variable="[^= ]+",
  " *= *",
  value="[^ ,]+"),
  by=chunk]

## Simple way to extract code chunks from Rnw.
vignette.Rnw &lt;- system.file(
  "extdata", "vignette.Rnw", package="nc")
Rnw.dt &lt;- nc::capture_all_str(
  vignette.Rnw,
  before=non.greedy.lines,
  "&lt;&lt;",
  name="[^,&gt;]*",
  parameters=".*",
  "&gt;&gt;=\n",
  code=non.greedy.lines,
  "@")
Rnw.dt[, .(name, parameters, some.code=substr(code, 1, 20))]

## The next example involves timing some compression programs that
## were run on a 159 megabyte input/uncompressed text file. Here is
## how to get a data table from the time command line output.
times.out &lt;- system.file(
  "extdata", "compress-times.out", package="nc", mustWork=TRUE)
times.dt &lt;- nc::capture_all_str(
  times.out,
  "coverage.bedGraph ",
  program=".*?",
  " coverage.bedGraph.",
  suffix=".*",
  "\n\nreal\t",
  minutes.only="[0-9]+", as.numeric,
  "m",
  seconds.only="[0-9.]+", as.numeric)
times.dt[, seconds := minutes.only*60+seconds.only]
times.dt

## join with output from du command line program.
sizes.out &lt;- system.file(
  "extdata", "compress-sizes.out", package="nc", mustWork=TRUE)
sizes.dt &lt;- data.table::fread(
  file=sizes.out,
  col.names=c("megabytes", "file"))
sizes.dt[, suffix := sub("coverage.bedGraph.?", "", file)]
join.dt &lt;- times.dt[sizes.dt, on="suffix"][order(megabytes)]
join.dt[file=="coverage.bedGraph", seconds := 0]
join.dt

## visualize with ggplot2.
if(require(ggplot2)){
  ggplot(join.dt, aes(
    seconds, megabytes, label=suffix))+
    geom_text(vjust=-0.5)+
    geom_point()+
    scale_x_log10()+
    scale_y_log10()
}

</code></pre>


</div>