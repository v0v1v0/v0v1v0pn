<div class="container">

<table style="width: 100%;"><tr>
<td>ncpen.reg</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>ncpen.reg: nonconvex penalized estimation</h2>

<h3>Description</h3>

<p>Fits generalized linear models by penalized maximum likelihood estimation.
The coefficients path is computed for the regression model over a grid of the regularization parameter <code>lambda</code>.
Fits Gaussian (linear), binomial Logit (logistic), Poisson, multinomial Logit regression models, and
Cox proportional hazard model with various non-convex penalties.
</p>


<h3>Usage</h3>

<pre><code class="language-R">ncpen.reg(formula, data, family = c("gaussian", "linear", "binomial",
  "logit", "multinomial", "cox", "poisson"), penalty = c("scad", "mcp",
  "tlp", "lasso", "classo", "ridge", "sridge", "mbridge", "mlog"),
  x.standardize = TRUE, intercept = TRUE, lambda = NULL,
  n.lambda = NULL, r.lambda = NULL, w.lambda = NULL, gamma = NULL,
  tau = NULL, alpha = NULL, df.max = 50, cf.max = 100,
  proj.min = 10, add.max = 10, niter.max = 30, qiter.max = 10,
  aiter.max = 100, b.eps = 1e-07, k.eps = 1e-04, c.eps = 1e-06,
  cut = TRUE, local = FALSE, local.initial = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>formula</code></td>
<td>
<p>(formula) regression formula. To include/exclude intercept, use <code>intercept</code> option
instead of using the "0 +" option in the formula.
The y value must be 0,1 for <code>binomial</code> and 1,2,..., for <code>multinomial</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>data</code></td>
<td>
<p>(numeric matrix or data.frame) contains both y and X. Each row is an observation vector.
The censoring indicator must be included at the last column of the data for <code>cox</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>family</code></td>
<td>
<p>(character) regression model. Supported models are
<code>gaussian</code> (or <code>linear</code>),
<code>binomial</code> (or <code>logit</code>),
<code>poisson</code>,
<code>multinomial</code>,
and <code>cox</code>.
Default is <code>gaussian</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>penalty</code></td>
<td>
<p>(character) penalty function.
Supported penalties are
<code>scad</code> (smoothly clipped absolute deviation),
<code>mcp</code> (minimax concave penalty),
<code>tlp</code> (truncated LASSO penalty),
<code>lasso</code> (least absolute shrinkage and selection operator),
<code>classo</code> (clipped lasso = mcp + lasso),
<code>ridge</code> (ridge),
<code>sridge</code> (sparse ridge = mcp + ridge),
<code>mbridge</code> (modified bridge) and
<code>mlog</code> (modified log).
Default is <code>scad</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>x.standardize</code></td>
<td>
<p>(logical) whether to standardize <code>x.mat</code> prior to fitting the model (see details).
The estimated coefficients are always restored to the original scale.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>intercept</code></td>
<td>
<p>(logical) whether to include an intercept in the model.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>lambda</code></td>
<td>
<p>(numeric vector) user-specified sequence of <code>lambda</code> values.
Default is supplied automatically from samples.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>n.lambda</code></td>
<td>
<p>(numeric) the number of <code>lambda</code> values.
Default is 100.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>r.lambda</code></td>
<td>
<p>(numeric) ratio of the smallest <code>lambda</code> value to largest.
Default is 0.001 when n&gt;p, and 0.01 for other cases.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>w.lambda</code></td>
<td>
<p>(numeric vector) penalty weights for each coefficient (see references).
If a penalty weight is set to 0, the corresponding coefficient is always nonzero.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>gamma</code></td>
<td>
<p>(numeric) additional tuning parameter for controlling shrinkage effect of <code>classo</code> and <code>sridge</code> (see references).
Default is half of the smallest <code>lambda</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>tau</code></td>
<td>
<p>(numeric) concavity parameter of the penalties (see reference).
Default is 3.7 for <code>scad</code>, 2.1 for <code>mcp</code>, <code>classo</code> and <code>sridge</code>, 0.001 for <code>tlp</code>, <code>mbridge</code> and <code>mlog</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>alpha</code></td>
<td>
<p>(numeric) ridge effect (weight between the penalty and ridge penalty) (see details).
Default value is 1. If penalty is <code>ridge</code> and <code>sridge</code> then <code>alpha</code> is set to 0.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>df.max</code></td>
<td>
<p>(numeric) the maximum number of nonzero coefficients.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>cf.max</code></td>
<td>
<p>(numeric) the maximum of absolute value of nonzero coefficients.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>proj.min</code></td>
<td>
<p>(numeric) the projection cycle inside CD algorithm (largely internal use. See details).</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>add.max</code></td>
<td>
<p>(numeric) the maximum number of variables added in CCCP iterations (largely internal use. See references).</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>niter.max</code></td>
<td>
<p>(numeric) maximum number of iterations in CCCP.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>qiter.max</code></td>
<td>
<p>(numeric) maximum number of quadratic approximations in each CCCP iteration.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>aiter.max</code></td>
<td>
<p>(numeric) maximum number of iterations in CD algorithm.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>b.eps</code></td>
<td>
<p>(numeric) convergence threshold for coefficients vector.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>k.eps</code></td>
<td>
<p>(numeric) convergence threshold for KKT conditions.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>c.eps</code></td>
<td>
<p>(numeric) convergence threshold for KKT conditions (largely internal use).</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>cut</code></td>
<td>
<p>(logical) convergence threshold for KKT conditions  (largely internal use).</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>local</code></td>
<td>
<p>(logical) whether to use local initial estimator for path construction. It may take a long time.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>local.initial</code></td>
<td>
<p>(numeric vector) initial estimator for <code>local=TRUE</code>.</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>The sequence of models indexed by <code>lambda</code> is fit
by using concave convex procedure (CCCP) and coordinate descent (CD) algorithm (see references).
The objective function is </p>
<p style="text-align: center;"><code class="reqn"> (sum of squared residuals)/2n + [alpha*penalty + (1-alpha)*ridge] </code>
</p>
<p> for <code>gaussian</code>
and </p>
<p style="text-align: center;"><code class="reqn"> (log-likelihood)/n - [alpha*penalty + (1-alpha)*ridge] </code>
</p>
<p> for the others,
assuming the canonical link.
The algorithm applies the warm start strategy (see references) and tries projections
after <code>proj.min</code> iterations in CD algorithm, which makes the algorithm fast and stable.
<code>x.standardize</code> makes each column of <code>x.mat</code> to have the same Euclidean length
but the coefficients will be re-scaled into the original.
In <code>multinomial</code> case, the coefficients are expressed in vector form. Use <code>coef.ncpen</code>.
</p>


<h3>Value</h3>

<p>An object with S3 class <code>ncpen</code>.
</p>
<table>
<tr style="vertical-align: top;">
<td><code>y.vec</code></td>
<td>
<p>response vector.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>x.mat</code></td>
<td>
<p>design matrix.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>family</code></td>
<td>
<p>regression model.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>penalty</code></td>
<td>
<p>penalty.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>x.standardize</code></td>
<td>
<p>whether to standardize <code>x.mat=TRUE</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>intercept</code></td>
<td>
<p>whether to include the intercept.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>std</code></td>
<td>
<p>scale factor for <code>x.standardize</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>lambda</code></td>
<td>
<p>sequence of <code>lambda</code> values.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>w.lambda</code></td>
<td>
<p>penalty weights.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>gamma</code></td>
<td>
<p>extra shrinkage parameter for <code>classo</code> and sridge only.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>alpha</code></td>
<td>
<p>ridge effect.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>local</code></td>
<td>
<p>whether to use local initial estimator.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>local.initial</code></td>
<td>
<p>local initial estimator for <code>local=TRUE</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>beta</code></td>
<td>
<p>fitted coefficients. Use <code>coef.ncpen</code> for <code>multinomial</code> since the coefficients are represented as vectors.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>df</code></td>
<td>
<p>the number of non-zero coefficients.</p>
</td>
</tr>
</table>
<h3>Author(s)</h3>

<p>Dongshin Kim, Sunghoon Kwon, Sangin Lee
</p>


<h3>References</h3>

<p>Fan, J. and Li, R. (2001). Variable selection via nonconcave penalized likelihood and its oracle properties.
<em>Journal of the American statistical Association</em>, 96, 1348-60.
Zhang, C.H. (2010). Nearly unbiased variable selection under minimax concave penalty.
<em>The Annals of statistics</em>, 38(2), 894-942.
Shen, X., Pan, W., Zhu, Y. and Zhou, H. (2013). On constrained and regularized high-dimensional regression.
<em>Annals of the Institute of Statistical Mathematics</em>, 65(5), 807-832.
Kwon, S., Lee, S. and Kim, Y. (2016). Moderately clipped LASSO.
<em>Computational Statistics and Data Analysis</em>, 92C, 53-67.
Kwon, S. Kim, Y. and Choi, H.(2013). Sparse bridge estimation with a diverging number of parameters.
<em>Statistics and Its Interface</em>, 6, 231-242.
Huang, J., Horowitz, J.L. and Ma, S. (2008). Asymptotic properties of bridge estimators in sparse high-dimensional regression models.
<em>The Annals of Statistics</em>, 36(2), 587-613.
Zou, H. and Li, R. (2008). One-step sparse estimates in nonconcave penalized likelihood models.
<em>Annals of statistics</em>, 36(4), 1509.
Lee, S., Kwon, S. and Kim, Y. (2016). A modified local quadratic approximation algorithm for penalized optimization problems.
<em>Computational Statistics and Data Analysis</em>, 94, 275-286.
</p>


<h3>See Also</h3>

<p><code>coef.ncpen</code>, <code>plot.ncpen</code>, <code>gic.ncpen</code>, <code>predict.ncpen</code>, <code>cv.ncpen</code>
</p>


<h3>Examples</h3>

<pre><code class="language-R">### linear regression with scad penalty
sam =  sam.gen.ncpen(n=200,p=5,q=5,cf.min=0.5,cf.max=1,corr=0.5,family="gaussian")
x.mat = sam$x.mat; y.vec = sam$y.vec
data = cbind(y.vec, x.mat)
colnames(data) = c("y", paste("xv", 1:ncol(x.mat), sep = ""))
fit1 = ncpen.reg(formula = y ~ xv1 + xv2 + xv3 + xv4 + xv5, data = data,
                 family="gaussian", penalty="scad")
fit2 = ncpen(y.vec=y.vec,x.mat=x.mat);

</code></pre>


</div>