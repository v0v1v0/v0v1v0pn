<div class="container">

<table style="width: 100%;"><tr>
<td>ncvreg</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Fit an MCP- or SCAD-penalized regression path</h2>

<h3>Description</h3>

<p>Fit coefficients paths for MCP- or SCAD-penalized regression models over a
grid of values for the regularization parameter lambda.  Fits linear and
logistic regression models, with option for an additional L2 penalty.
</p>


<h3>Usage</h3>

<pre><code class="language-R">ncvreg(
  X,
  y,
  family = c("gaussian", "binomial", "poisson"),
  penalty = c("MCP", "SCAD", "lasso"),
  gamma = switch(penalty, SCAD = 3.7, 3),
  alpha = 1,
  lambda.min = ifelse(n &gt; p, 0.001, 0.05),
  nlambda = 100,
  lambda,
  eps = 1e-04,
  max.iter = 10000,
  convex = TRUE,
  dfmax = p + 1,
  penalty.factor = rep(1, ncol(X)),
  warn = TRUE,
  returnX,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>X</code></td>
<td>
<p>The design matrix, without an intercept.  <code>ncvreg</code>
standardizes the data and includes an intercept by default.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>y</code></td>
<td>
<p>The response vector.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>family</code></td>
<td>
<p>Either "gaussian", "binomial", or "poisson", depending on
the response.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>penalty</code></td>
<td>
<p>The penalty to be applied to the model.  Either "MCP" (the
default), "SCAD", or "lasso".</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>gamma</code></td>
<td>
<p>The tuning parameter of the MCP/SCAD penalty (see details).
Default is 3 for MCP and 3.7 for SCAD.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>alpha</code></td>
<td>
<p>Tuning parameter for the Mnet estimator which controls the
relative contributions from the MCP/SCAD penalty and the
ridge, or L2 penalty. <code>alpha=1</code> is equivalent to MCP/SCAD
penalty, while <code>alpha=0</code> would be equivalent to ridge
regression. However, <code>alpha=0</code> is not supported; <code>alpha</code> may
be arbitrarily small, but not exactly 0.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>lambda.min</code></td>
<td>
<p>The smallest value for lambda, as a fraction of
lambda.max. Default is 0.001 if the number of observations
is larger than the number of covariates and .05 otherwise.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>nlambda</code></td>
<td>
<p>The number of lambda values. Default is 100.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>lambda</code></td>
<td>
<p>A user-specified sequence of lambda values.  By default, a
sequence of values of length <code>nlambda</code> is computed, equally
spaced on the log scale.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>eps</code></td>
<td>
<p>Convergence threshhold.  The algorithm iterates until the
RMSD for the change in linear predictors for each
coefficient is less than <code>eps</code>. Default is <code>1e-4</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>max.iter</code></td>
<td>
<p>Maximum number of iterations (total across entire path).
Default is 10000.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>convex</code></td>
<td>
<p>Calculate index for which objective function ceases to be
locally convex? Default is TRUE.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>dfmax</code></td>
<td>
<p>Upper bound for the number of nonzero coefficients.  Default
is no upper bound. However, for large data sets,
computational burden may be heavy for models with a large
number of nonzero coefficients.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>penalty.factor</code></td>
<td>
<p>A multiplicative factor for the penalty applied to
each coefficient. If supplied, <code>penalty.factor</code> must be
a numeric vector of length equal to the number of
columns of <code>X</code>.  The purpose of <code>penalty.factor</code> is to
apply differential penalization if some coefficients
are thought to be more likely than others to be in the
model. In particular, <code>penalty.factor</code> can be 0, in
which case the coefficient is always in the model
without shrinkage.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>warn</code></td>
<td>
<p>Return warning messages for failures to converge and model
saturation?  Default is TRUE.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>returnX</code></td>
<td>
<p>Return the standardized design matrix along with the fit? By
default, this option is turned on if X is under 100 MB, but
turned off for larger matrices to preserve memory. Note that
certain methods, such as <code>summary.ncvreg()</code> require access
to the design matrix and may not be able to run if
<code>returnX=FALSE</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>...</code></td>
<td>
<p>Not used.</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>The sequence of models indexed by the regularization parameter <code>lambda</code> is
fit using a coordinate descent algorithm.  For logistic regression models,
some care is taken to avoid model saturation; the algorithm may exit early in
this setting.  The objective function is defined to be
</p>
<p style="text-align: center;"><code class="reqn">Q(\beta|X, y) = \frac{1}{n} L(\beta|X, y) + P_\lambda(\beta),</code>
</p>

<p>where the loss function L is the deviance (-2 times the log likelihood) for
the specified outcome distribution (gaussian/binomial/poisson). See
<a href="https://pbreheny.github.io/ncvreg/articles/web/models.html">here</a> for more
details.
</p>
<p>This algorithm is stable, very efficient, and generally converges quite
rapidly to the solution.  For GLMs,
<a href="https://pbreheny.github.io/ncvreg/articles/web/adaptive-rescaling.html">adaptive rescaling</a>
is used.
</p>


<h3>Value</h3>

<p>An object with S3 class <code>"ncvreg"</code> containing:
</p>

<dl>
<dt>beta</dt>
<dd>
<p>The fitted matrix of coefficients. The number of rows is equal to the number of coefficients, and the number of columns is equal to <code>nlambda</code>.</p>
</dd>
<dt>iter</dt>
<dd>
<p>A vector of length <code>nlambda</code> containing the number of iterations until convergence at each value of <code>lambda</code>.</p>
</dd>
<dt>lambda</dt>
<dd>
<p>The sequence of regularization parameter values in the path.</p>
</dd>
<dt>penalty, family, gamma, alpha, penalty.factor</dt>
<dd>
<p>Same as above.</p>
</dd>
<dt>convex.min</dt>
<dd>
<p>The last index for which the objective function is locally convex. The smallest value of lambda for which the objective function is convex is therefore <code>lambda[convex.min]</code>, with corresponding coefficients <code>beta[,convex.min]</code>.</p>
</dd>
<dt>loss</dt>
<dd>
<p>A vector containing the deviance (i.e., the loss) at each value of <code>lambda</code>. Note that for <code>gaussian</code> models, the loss is simply the residual sum of squares.</p>
</dd>
<dt>n</dt>
<dd>
<p>Sample size.</p>
</dd>
</dl>
<p>Additionally, if <code>returnX=TRUE</code>, the object will also contain
</p>

<dl>
<dt>X</dt>
<dd>
<p>The standardized design matrix.</p>
</dd>
<dt>y</dt>
<dd>
<p>The response, centered if <code>family='gaussian'</code>.</p>
</dd>
</dl>
<h3>References</h3>

<p>Breheny P and Huang J. (2011) Coordinate descent algorithms for nonconvex
penalized regression, with applications to biological feature selection.
<em>Annals of Applied Statistics</em>, <strong>5</strong>: 232-253.
<a href="https://doi.org/10.1214/10-AOAS388">doi:10.1214/10-AOAS388</a>
</p>


<h3>See Also</h3>

<p><code>plot.ncvreg()</code>, <code>cv.ncvreg()</code>
</p>


<h3>Examples</h3>

<pre><code class="language-R"># Linear regression --------------------------------------------------
data(Prostate)
X &lt;- Prostate$X
y &lt;- Prostate$y

op &lt;- par(mfrow=c(2,2))
fit &lt;- ncvreg(X, y)
plot(fit, main=expression(paste(gamma,"=",3)))
fit &lt;- ncvreg(X, y, gamma=10)
plot(fit, main=expression(paste(gamma,"=",10)))
fit &lt;- ncvreg(X, y, gamma=1.5)
plot(fit, main=expression(paste(gamma,"=",1.5)))
fit &lt;- ncvreg(X, y, penalty="SCAD")
plot(fit, main=expression(paste("SCAD, ",gamma,"=",3)))
par(op)

op &lt;- par(mfrow=c(2,2))
fit &lt;- ncvreg(X, y)
plot(fit, main=expression(paste(alpha,"=",1)))
fit &lt;- ncvreg(X, y, alpha=0.9)
plot(fit, main=expression(paste(alpha,"=",0.9)))
fit &lt;- ncvreg(X, y, alpha=0.5)
plot(fit, main=expression(paste(alpha,"=",0.5)))
fit &lt;- ncvreg(X, y, alpha=0.1)
plot(fit, main=expression(paste(alpha,"=",0.1)))
par(op)

op &lt;- par(mfrow=c(2,2))
fit &lt;- ncvreg(X, y)
plot(mfdr(fit))             # Independence approximation
plot(mfdr(fit), type="EF")  # Independence approximation
perm.fit &lt;- perm.ncvreg(X, y)
plot(perm.fit)
plot(perm.fit, type="EF")
par(op)

# Logistic regression ------------------------------------------------
data(Heart)
X &lt;- Heart$X
y &lt;- Heart$y

op &lt;- par(mfrow=c(2,2))
fit &lt;- ncvreg(X, y, family="binomial")
plot(fit, main=expression(paste(gamma,"=",3)))
fit &lt;- ncvreg(X, y, family="binomial", gamma=10)
plot(fit, main=expression(paste(gamma,"=",10)))
fit &lt;- ncvreg(X, y, family="binomial", gamma=1.5)
plot(fit, main=expression(paste(gamma,"=",1.5)))
fit &lt;- ncvreg(X, y, family="binomial", penalty="SCAD")
plot(fit, main=expression(paste("SCAD, ",gamma,"=",3)))
par(op)

op &lt;- par(mfrow=c(2,2))
fit &lt;- ncvreg(X, y, family="binomial")
plot(fit, main=expression(paste(alpha,"=",1)))
fit &lt;- ncvreg(X, y, family="binomial", alpha=0.9)
plot(fit, main=expression(paste(alpha,"=",0.9)))
fit &lt;- ncvreg(X, y, family="binomial", alpha=0.5)
plot(fit, main=expression(paste(alpha,"=",0.5)))
fit &lt;- ncvreg(X, y, family="binomial", alpha=0.1)
plot(fit, main=expression(paste(alpha,"=",0.1)))
par(op)

</code></pre>


</div>