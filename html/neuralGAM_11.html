<div class="container">

<table style="width: 100%;"><tr>
<td>neuralGAM</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Fit a <code>neuralGAM</code> model</h2>

<h3>Description</h3>

<p>Fits a <code>neuralGAM</code> model by building a neural network to attend to each covariate.
</p>


<h3>Usage</h3>

<pre><code class="language-R">neuralGAM(
  formula,
  data,
  num_units,
  family = "gaussian",
  learning_rate = 0.001,
  activation = "relu",
  kernel_initializer = "glorot_normal",
  kernel_regularizer = NULL,
  bias_regularizer = NULL,
  bias_initializer = "zeros",
  activity_regularizer = NULL,
  loss = "mse",
  w_train = NULL,
  bf_threshold = 0.001,
  ls_threshold = 0.1,
  max_iter_backfitting = 10,
  max_iter_ls = 10,
  seed = NULL,
  verbose = 1,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>formula</code></td>
<td>
<p>An object of class "formula": a description of the model to be fitted. You can add smooth terms using <code>s()</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>data</code></td>
<td>
<p>A data frame containing the model response variable and covariates
required by the formula. Additional terms not present in the formula will be ignored.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>num_units</code></td>
<td>
<p>Defines the architecture of each neural network.
If a scalar value is provided, a single hidden layer neural network with that number of units is used.
If a vector of values is provided, a multi-layer neural network with each element of the vector defining
the number of hidden units on each hidden layer is used.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>family</code></td>
<td>
<p>This is a family object specifying the distribution and link to use for fitting.
By default, it is <code>"gaussian"</code> but also works to <code>"binomial"</code> for logistic regression.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>learning_rate</code></td>
<td>
<p>Learning rate for the neural network optimizer.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>activation</code></td>
<td>
<p>Activation function of the neural network. Defaults to <code>relu</code></p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>kernel_initializer</code></td>
<td>
<p>Kernel initializer for the Dense layers.
Defaults to Xavier Initializer (<code>glorot_normal</code>).</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>kernel_regularizer</code></td>
<td>
<p>Optional regularizer function applied to the kernel weights matrix.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>bias_regularizer</code></td>
<td>
<p>Optional regularizer function applied to the bias vector.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>bias_initializer</code></td>
<td>
<p>Optional initializer for the bias vector.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>activity_regularizer</code></td>
<td>
<p>Optional regularizer function applied to the output of the layer</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>loss</code></td>
<td>
<p>Loss function to use during neural network training. Defaults to the mean squared error.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>w_train</code></td>
<td>
<p>Optional sample weights</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>bf_threshold</code></td>
<td>
<p>Convergence criterion of the backfitting algorithm.
Defaults to <code>0.001</code></p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>ls_threshold</code></td>
<td>
<p>Convergence criterion of the local scoring algorithm.
Defaults to <code>0.1</code></p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>max_iter_backfitting</code></td>
<td>
<p>An integer with the maximum number of iterations
of the backfitting algorithm. Defaults to <code>10</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>max_iter_ls</code></td>
<td>
<p>An integer with the maximum number of iterations of the
local scoring Algorithm. Defaults to <code>10</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>seed</code></td>
<td>
<p>A positive integer which specifies the random number generator
seed for algorithms dependent on randomization.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>verbose</code></td>
<td>
<p>Verbosity mode (0 = silent, 1 = print messages). Defaults to 1.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>...</code></td>
<td>
<p>Additional parameters for the Adam optimizer (see ?keras::optimizer_adam)</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>The function builds one neural network to attend to each feature in x,
using the backfitting and local scoring algorithms to fit a weighted additive model
using neural networks as function approximators. The adjustment of the
dependent variable and the weights is determined by the distribution of the
response <code>y</code>, adjusted by the <code>family</code> parameter.
</p>


<h3>Value</h3>

<p>A trained <code>neuralGAM</code> object. Use <code>summary(ngam)</code> to see details.
</p>


<h3>Author(s)</h3>

<p>Ines Ortega-Fernandez, Marta Sestelo.
</p>


<h3>References</h3>

<p>Hastie, T., &amp; Tibshirani, R. (1990). Generalized Additive Models. London: Chapman and Hall, 1931(11), 683â€“741.
</p>


<h3>Examples</h3>

<pre><code class="language-R">## Not run: 
n &lt;- 24500

seed &lt;- 42
set.seed(seed)

x1 &lt;- runif(n, -2.5, 2.5)
x2 &lt;- runif(n, -2.5, 2.5)
x3 &lt;- runif(n, -2.5, 2.5)

f1 &lt;- x1 ** 2
f2 &lt;- 2 * x2
f3 &lt;- sin(x3)
f1 &lt;- f1 - mean(f1)
f2 &lt;- f2 - mean(f2)
f3 &lt;- f3 - mean(f3)

eta0 &lt;- 2 + f1 + f2 + f3
epsilon &lt;- rnorm(n, 0.25)
y &lt;- eta0 + epsilon
train &lt;- data.frame(x1, x2, x3, y)

library(neuralGAM)
ngam &lt;- neuralGAM(y ~ s(x1) + x2 + s(x3), data = train,
                 num_units = 1024, family = "gaussian",
                 activation = "relu",
                 learning_rate = 0.001, bf_threshold = 0.001,
                 max_iter_backfitting = 10, max_iter_ls = 10,
                 seed = seed
                 )

ngam

## End(Not run)
</code></pre>


</div>