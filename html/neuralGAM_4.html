<div class="container">

<table style="width: 100%;"><tr>
<td>build_feature_NN</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Build and compile a single Neural Network</h2>

<h3>Description</h3>

<p>Builds and compiles a neural network using the keras library.
The architecture of the neural network is configurable using the
</p>


<h3>Usage</h3>

<pre><code class="language-R">build_feature_NN(
  num_units,
  learning_rate = 0.001,
  activation = "relu",
  kernel_initializer = "glorot_normal",
  kernel_regularizer = NULL,
  bias_regularizer = NULL,
  bias_initializer = "zeros",
  activity_regularizer = NULL,
  loss = "mean_squared_error",
  name = NULL,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>num_units</code></td>
<td>
<p>Defines the architecture of each neural network.
If a scalar value is provided, a single hidden layer neural network with that number of units is used.
If a vector of values is provided, a multi-layer neural network with each element of the vector defining
the number of hidden units on each hidden layer is used.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>learning_rate</code></td>
<td>
<p>Learning rate for the neural network optimizer.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>activation</code></td>
<td>
<p>Activation function of the neural network. Defaults to <code>relu</code></p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>kernel_initializer</code></td>
<td>
<p>Kernel initializer for the Dense layers.
Defaults to Xavier Initializer (<code>glorot_normal</code>).</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>kernel_regularizer</code></td>
<td>
<p>Optional regularizer function applied to the kernel weights matrix.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>bias_regularizer</code></td>
<td>
<p>Optional regularizer function applied to the bias vector.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>bias_initializer</code></td>
<td>
<p>Optional initializer for the bias vector.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>activity_regularizer</code></td>
<td>
<p>Optional regularizer function applied to the output of the layer</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>loss</code></td>
<td>
<p>Loss function to use during neural network training. Defaults to the mean squared error.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>name</code></td>
<td>
<p>Neural Network name.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>...</code></td>
<td>

<p>Arguments passed on to <code>neuralGAM</code>
</p>

<dl>
<dt><code>formula</code></dt>
<dd>
<p>An object of class "formula": a description of the model to be fitted. You can add smooth terms using <code>s()</code>.</p>
</dd>
<dt><code>data</code></dt>
<dd>
<p>A data frame containing the model response variable and covariates
required by the formula. Additional terms not present in the formula will be ignored.</p>
</dd>
<dt><code>family</code></dt>
<dd>
<p>This is a family object specifying the distribution and link to use for fitting.
By default, it is <code>"gaussian"</code> but also works to <code>"binomial"</code> for logistic regression.</p>
</dd>
<dt><code>bf_threshold</code></dt>
<dd>
<p>Convergence criterion of the backfitting algorithm.
Defaults to <code>0.001</code></p>
</dd>
<dt><code>ls_threshold</code></dt>
<dd>
<p>Convergence criterion of the local scoring algorithm.
Defaults to <code>0.1</code></p>
</dd>
<dt><code>max_iter_backfitting</code></dt>
<dd>
<p>An integer with the maximum number of iterations
of the backfitting algorithm. Defaults to <code>10</code>.</p>
</dd>
<dt><code>max_iter_ls</code></dt>
<dd>
<p>An integer with the maximum number of iterations of the
local scoring Algorithm. Defaults to <code>10</code>.</p>
</dd>
<dt><code>w_train</code></dt>
<dd>
<p>Optional sample weights</p>
</dd>
<dt><code>seed</code></dt>
<dd>
<p>A positive integer which specifies the random number generator
seed for algorithms dependent on randomization.</p>
</dd>
<dt><code>verbose</code></dt>
<dd>
<p>Verbosity mode (0 = silent, 1 = print messages). Defaults to 1.</p>
</dd>
</dl>
</td>
</tr>
</table>
<h3>Value</h3>

<p>compiled Neural Network
</p>


<h3>Author(s)</h3>

<p>Ines Ortega-Fernandez, Marta Sestelo.
</p>


<h3>References</h3>

<p>Kingma, D. P., &amp; Ba, J. (2014). Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980.
</p>


</div>