<div class="container">

<table style="width: 100%;"><tr>
<td>neuralnet-package</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Training of Neural Networks</h2>

<h3>Description</h3>

<p>Training of neural networks using the backpropagation, resilient
backpropagation with (Riedmiller, 1994) or without weight backtracking
(Riedmiller, 1993) or the modified globally convergent version by
Anastasiadis et al. (2005). The package allows flexible settings through
custom-choice of error and activation function. Furthermore, the calculation
of generalized weights (Intrator O &amp; Intrator N, 1993) is implemented.
</p>


<h3>Note</h3>

<p>This work has been supported by the German Research Foundation<br>
(DFG: <a href="http://www.dfg.de">http://www.dfg.de</a>) under grant scheme PI 345/3-1.
</p>


<h3>Author(s)</h3>

<p>Stefan Fritsch, Frauke Guenther <a href="mailto:guenther@leibniz-bips.de">guenther@leibniz-bips.de</a>,
</p>
<p>Maintainer: Frauke Guenther <a href="mailto:guenther@leibniz-bips.de">guenther@leibniz-bips.de</a>
</p>


<h3>References</h3>

<p>Riedmiller M. (1994) <em>Rprop - Description and
Implementation Details.</em> Technical Report. University of Karlsruhe.
</p>
<p>Riedmiller M. and Braun H. (1993) <em>A direct adaptive method for faster
backpropagation learning: The RPROP algorithm.</em> Proceedings of the IEEE
International Conference on Neural Networks (ICNN), pages 586-591.  San
Francisco.
</p>
<p>Anastasiadis A. et. al. (2005) <em>New globally convergent training scheme
based on the resilient propagation algorithm.</em> Neurocomputing 64, pages
253-270.
</p>
<p>Intrator O. and Intrator N. (1993) <em>Using Neural Nets for
Interpretation of Nonlinear Models.</em> Proceedings of the Statistical
Computing Section, 244-249 San Francisco: American Statistical Society
(eds).
</p>


<h3>See Also</h3>

<p><code>plot.nn</code> for plotting of the neural network.
</p>
<p><code>gwplot</code> for plotting of the generalized weights.
</p>
<p><code>compute</code> for computation of the calculated network.
</p>
<p><code>confidence.interval</code> for calculation of a confidence interval
for the weights.
</p>
<p><code>prediction</code> for calculation of a prediction.
</p>


<h3>Examples</h3>

<pre><code class="language-R">
AND &lt;- c(rep(0,7),1)
OR &lt;- c(0,rep(1,7))
binary.data &lt;- data.frame(expand.grid(c(0,1), c(0,1), c(0,1)), AND, OR)
print(net &lt;- neuralnet(AND+OR~Var1+Var2+Var3,  binary.data, hidden=0, 
             rep=10, err.fct="ce", linear.output=FALSE))

XOR &lt;- c(0,1,1,0)
xor.data &lt;- data.frame(expand.grid(c(0,1), c(0,1)), XOR)
print(net.xor &lt;- neuralnet(XOR~Var1+Var2, xor.data, hidden=2, rep=5))
plot(net.xor, rep="best")

data(infert, package="datasets")
print(net.infert &lt;- neuralnet(case~parity+induced+spontaneous,  infert, 
                    err.fct="ce", linear.output=FALSE, likelihood=TRUE))
gwplot(net.infert, selected.covariate="parity")
gwplot(net.infert, selected.covariate="induced")
gwplot(net.infert, selected.covariate="spontaneous")
confidence.interval(net.infert)

Var1 &lt;- runif(50, 0, 100) 
sqrt.data &lt;- data.frame(Var1, Sqrt=sqrt(Var1))
print(net.sqrt &lt;- neuralnet(Sqrt~Var1, sqrt.data, hidden=10, 
                  threshold=0.01))
predict(net.sqrt, data.frame(Var1 = (1:10)^2))

Var1 &lt;- rpois(100,0.5)
Var2 &lt;- rbinom(100,2,0.6)
Var3 &lt;- rbinom(100,1,0.5)
SUM &lt;- as.integer(abs(Var1+Var2+Var3+(rnorm(100))))
sum.data &lt;- data.frame(Var1,Var2,Var3, SUM)
print(net.sum &lt;- neuralnet(SUM~Var1+Var2+Var3, sum.data, hidden=1, 
                 act.fct="tanh"))
prediction(net.sum)

</code></pre>


</div>