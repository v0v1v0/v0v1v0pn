<div class="container">

<table style="width: 100%;"><tr>
<td>nmfbin</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Logistic Non-negative Matrix Factorization</h2>

<h3>Description</h3>

<p>This function performs Logistic Non-negative Matrix Factorization (NMF) on a binary matrix.
</p>


<h3>Usage</h3>

<pre><code class="language-R">nmfbin(
  X,
  k,
  optimizer = "mur",
  init = "nndsvd",
  max_iter = 1000,
  tol = 1e-06,
  learning_rate = 0.001,
  verbose = FALSE,
  loss_fun = "logloss",
  loss_normalize = TRUE,
  epsilon = 1e-10
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>X</code></td>
<td>
<p>A binary matrix (m x n) to be factorized.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>k</code></td>
<td>
<p>The number of factors (components, topics).</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>optimizer</code></td>
<td>
<p>Type of updating algorithm. <code>mur</code> for NMF multiplicative update rules, <code>gradient</code> for gradient descent, <code>sgd</code> for stochastic gradient descent.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>init</code></td>
<td>
<p>Method for initializing the factorization. By default Nonnegative Double Singular Value Decomposition with average densification.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>max_iter</code></td>
<td>
<p>Maximum number of iterations for optimization.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>tol</code></td>
<td>
<p>Convergence tolerance. The optimization stops when the change in loss is less than this value.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>learning_rate</code></td>
<td>
<p>Learning rate (step size) for the gradient descent optimization.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>verbose</code></td>
<td>
<p>Print convergence if <code>TRUE</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>loss_fun</code></td>
<td>
<p>Choice of loss function: <code>logloss</code> (negative log-likelihood, also known as binary cross-entropy) or <code>mse</code> (mean squared error).</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>loss_normalize</code></td>
<td>
<p>Normalize loss by matrix dimensions if <code>TRUE</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>epsilon</code></td>
<td>
<p>Constant to avoid log(0).</p>
</td>
</tr>
</table>
<h3>Value</h3>

<p>A list containing:
</p>

<ul>
<li> <p><code>W</code>: The basis matrix (m x k). The document-topic matrix in topic modelling.
</p>
</li>
<li> <p><code>H</code>: The coefficient matrix (k x n). Contribution of features to factors (topics).
</p>
</li>
<li> <p><code>c</code>: The global threshold. A constant.
</p>
</li>
<li> <p><code>convergence</code>: Divergence (loss) from <code>X</code> at every <code>iter</code> until <code>tol</code> or <code>max_iter</code> is reached.
</p>
</li>
</ul>
<h3>Examples</h3>

<pre><code class="language-R"># Generate a binary matrix
m &lt;- 100
n &lt;- 50
X &lt;- matrix(sample(c(0, 1), m * n, replace = TRUE), m, n)

# Set the number of factors
k &lt;- 4

# Factorize the matrix with default settings
result &lt;- nmfbin(X, k)
</code></pre>


</div>