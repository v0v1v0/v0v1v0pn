<div class="container">

<table style="width: 100%;"><tr>
<td>inst</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>inst</h2>

<h3>Description</h3>

<p>The function that instantiates a neural network as created
by create_nn().
</p>


<h3>Usage</h3>

<pre><code class="language-R">inst(neural_network, activation_function, x)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>neural_network</code></td>
<td>
<p>An ordered list of lists, of the type generated by
create_nn() where each element in the
list of lists is a pair <code class="reqn">(W,b)</code> representing the weights and biases of
that layer.
</p>
<p><em>NOTE:</em> We will call istantiation what Grohs et. al. call "realization".</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>activation_function</code></td>
<td>
<p>A continuous function applied to the output of each layer. For now we only
have ReLU, Sigmoid, and Tanh. Note, all proofs are only valid for ReLU activation.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>x</code></td>
<td>
<p>our input to the continuous function formed from activation. Our input will
be an element in <code class="reqn">\mathbb{R}^d</code> for some appropriate <code class="reqn">d</code>.</p>
</td>
</tr>
</table>
<h3>Value</h3>

<p>The output of the continuous function that is the instantiation of the given
neural network with the given activation function at the given <code class="reqn">x</code>. Where <code class="reqn">x</code>
is of vector size equal to the input layer of the neural network.
</p>


<h3>References</h3>

<p>Grohs, P., Hornung, F., Jentzen, A. et al. Space-time error estimates for deep
neural network approximations for differential equations. (2019).
<a href="https://arxiv.org/abs/1908.03833">https://arxiv.org/abs/1908.03833</a>.
</p>
<p>Definition 1.3.4. Jentzen, A., Kuckuck, B., and von Wurstemberger, P. (2023).
Mathematical introduction to deep learning: Methods, implementations,
and theory. <a href="https://arxiv.org/abs/2310.20360">https://arxiv.org/abs/2310.20360</a>
</p>
<p>Very precisely we will use the definition in:
</p>
<p>Definition 2.3 in Rafi S., Padgett, J.L., Nakarmi, U. (2024) Towards an Algebraic Framework For
Approximating Functions Using Neural Network Polynomials
<a href="https://arxiv.org/abs/2402.01058">https://arxiv.org/abs/2402.01058</a>
</p>


<h3>Examples</h3>

<pre><code class="language-R">create_nn(c(1, 3, 5, 6)) |&gt; inst(ReLU, 5)
create_nn(c(3, 3, 5, 6)) |&gt; inst(ReLU, c(4, 4, 4))

</code></pre>


</div>