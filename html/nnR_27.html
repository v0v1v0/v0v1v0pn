<div class="container">

<table style="width: 100%;"><tr>
<td>MC</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>The MC neural network</h2>

<h3>Description</h3>

<p>This function implements the 1-D approximation scheme outlined in the References.
</p>
<p><strong>Note:</strong> Only 1-D interpolation is implemented.
</p>


<h3>Usage</h3>

<pre><code class="language-R">MC(X, y, L)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>X</code></td>
<td>
<p>a list of samples from the functions domain.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>y</code></td>
<td>
<p>the function applied componentwise to each point in the domain.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>L</code></td>
<td>
<p>the Lipschitz constant for the function. Not necessarily global,
but could be an absolute upper limit of slope, over the domain.</p>
</td>
</tr>
</table>
<h3>Value</h3>

<p>A neural network that gives the maximum convolution approximation
of a function whose outputs is <code class="reqn">y</code> at <code class="reqn">n</code> sample points given by
each row of <code class="reqn">X</code>, when instantiated with ReLU.
</p>


<h3>References</h3>

<p>Lemma 4.2.9. Jentzen, A., Kuckuck, B., and von Wurstemberger, P. (2023).
Mathematical introduction to deep learning: Methods, implementations,
and theory. <a href="https://arxiv.org/abs/2310.20360">https://arxiv.org/abs/2310.20360</a>.
</p>


<h3>Examples</h3>

<pre><code class="language-R">
seq(0, 3.1416, length.out = 200) -&gt; X
sin(X) -&gt; y
MC(X, y, 1) |&gt; inst(ReLU, 0.25) # compare to sin(0.25)

</code></pre>


</div>