<div class="container">

<table style="width: 100%;"><tr>
<td>stk</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>stk</h2>

<h3>Description</h3>

<p>A function that stacks neural networks.
</p>


<h3>Usage</h3>

<pre><code class="language-R">stk(nu, mu)

nu %stk% mu
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>nu</code></td>
<td>
<p>neural network.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>mu</code></td>
<td>
<p>neural network.</p>
</td>
</tr>
</table>
<h3>Value</h3>

<p>A stacked neural network of <code class="reqn">\nu</code> and <code class="reqn">\mu</code>, i.e. <code class="reqn">\nu \boxminus \mu</code>
</p>
<p><strong>NOTE:</strong> This is different than the one given in Grohs, et. al. 2023.
While we use padding to equalize neural networks being parallelized our
padding is via the Tun network whereas Grohs et. al. uses repetitive
composition of the i network. We use repetitive composition of the <code class="reqn">\mathsf{Id_1}</code>
network. See <code>Id</code> <code>comp</code>
</p>
<p><strong>NOTE:</strong> The terminology is also different from Grohs et. al. 2023.
We call stacking what they call parallelization. This terminology change was
inspired by the fact that parallelization implies commutativity but this
operation is not quite commutative. It is commutative up to transposition
of our input x under instantiation with a continuous activation function.
</p>
<p>Also the word parallelization has a lot of baggage when it comes to
artificial neural networks in that it often means many different CPUs working
together.
</p>
<p><em>Remark:</em> We will use only one symbol for stacking equal and unequal depth
neural networks, namely "stk". This is for usability but also that
for all practical purposes only the general stacking of neural networks
of different sizes is what is needed.
</p>
<p><em>Remark:</em> We have two versions, a prefix and an infix version.
</p>
<p>This operation on neural networks, called "parallelization" is found in:
</p>
<p>A stacked neural network of nu and mu.
</p>


<h3>References</h3>

<p>Grohs, P., Hornung, F., Jentzen, A. et al. Space-time error estimates for deep
neural network approximations for differential equations. (2023).
<a href="https://arxiv.org/abs/1908.03833">https://arxiv.org/abs/1908.03833</a>
</p>
<p>And especially in:
</p>
<p>' Definition 2.14 in Rafi S., Padgett, J.L., Nakarmi, U. (2024)
Towards an Algebraic Framework For
Approximating Functions Using Neural Network Polynomials
<a href="https://arxiv.org/abs/2402.01058">https://arxiv.org/abs/2402.01058</a>
</p>


<h3>Examples</h3>

<pre><code class="language-R">create_nn(c(4,5,6)) |&gt; stk(create_nn(c(6,7)))
create_nn(c(9,1,67)) |&gt; stk(create_nn(c(4,4,4,4,4)))


</code></pre>


</div>