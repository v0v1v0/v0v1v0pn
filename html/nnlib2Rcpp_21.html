<div class="container">

<table style="width: 100%;"><tr>
<td>BP-class</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Class <code>"BP"</code>
</h2>

<h3>Description</h3>

<p>Supervised Back-Propagation (BP) NN module, for encoding input-output mappings.
</p>


<h3>Extends</h3>

<p>Class <code>"RcppClass"</code>, directly.
</p>
<p>All reference classes extend and inherit methods from <code>"envRefClass"</code>.
</p>


<h3>Fields</h3>


<dl>
<dt>
<code>.CppObject</code>:</dt>
<dd>
<p>Object of class <code>C++Object</code> ~~ </p>
</dd>
<dt>
<code>.CppClassDef</code>:</dt>
<dd>
<p>Object of class <code>activeBindingFunction</code> ~~ </p>
</dd>
<dt>
<code>.CppGenerator</code>:</dt>
<dd>
<p>Object of class <code>activeBindingFunction</code> ~~ </p>
</dd>
</dl>
<h3>Methods</h3>


<dl>
<dt>
<code>encode( data_in, data_out, learning_rate, training_epochs, hidden_layers, hidden_layer_size )</code>:</dt>
<dd>
<p> Setup a new BP NN and encode input-output data pairs. Parameters are:
</p>

<ul>
<li>
<p><code>data_in</code>: numeric matrix, containing input vectors as rows. . It is recommended that these values are in 0 to 1 range.
</p>
</li>
<li>
<p><code>data_out</code>: numeric matrix, containing corresponding (desired) output vectors. It is recommended that these values are in 0 to 1 range.
</p>
</li>
<li>
<p><code>learning_rate</code>: a number (preferably greater than 0 and less than 1) used in training.
</p>
</li>
<li>
<p><code>training_epochs</code>: number of training epochs, aka single presentation iterations of all training data pairs to the NN during training.
</p>
</li>
<li>
<p><code>hidden_layers</code>: number of hidden layers to be created between input and output layers.
</p>
</li>
<li>
<p><code>hidden_layer_size</code>: number of nodes (processing elements or PEs) in each of the hidden layers (all hidden layers are of the same length in this implementation of BP).
</p>
</li>
</ul>
<p>Note: to encode additional input-output vector pairs in an existing BP, use <code>train_single</code> or <code>train_multiple</code> methods (see below).
</p>
</dd>
<dt>
<code>recall(data_in)</code>:</dt>
<dd>
<p> Get output for a dataset (numeric matrix <code>data_in</code>) from the (trained) BP NN. </p>
</dd>
<dt>
<code>setup(input_dim, output_dim, learning_rate, hidden_layers, hidden_layer_size)</code>:</dt>
<dd>
<p> Setup the BP NN so it can be trained and used. Note: this is not needed if using <code>encode</code>. Parameters are:
</p>

<ul>
<li>
<p><code>input_dim</code>: integer length of input vectors.
</p>
</li>
<li>
<p><code>output_dim</code>: integer length of output vectors.
</p>
</li>
<li>
<p><code>learning_rate</code>: a number (preferably greater than 0 and less than 1) used in training.
</p>
</li>
<li>
<p><code>hidden_layers</code>: number of hidden layers to be created between input and output layers.
</p>
</li>
<li>
<p><code>hidden_layer_size</code>: number of nodes (processing elements or PEs) in each of the hidden layers (all hidden layers are of the same length in this implementation of BP).
</p>
</li>
</ul>
</dd>
<dt>
<code>train_single (data_in, data_out)</code>:</dt>
<dd>
<p> Encode an input-output vector pair in the BP NN. Only performs a single training iteration (multiple may be required for proper encoding). Vector sizes should be compatible to the current NN (as resulted from the <code>encode</code> or <code>setup</code> methods). Returns error level indicator value.</p>
</dd>
<dt>
<code>train_multiple (data_in, data_out, training_epochs)</code>:</dt>
<dd>
<p> Encode multiple input-output vector pairs stored in corresponding datasets. Performs multiple iterations in epochs (see <code>encode</code>). Vector sizes should be compatible to the current NN (as resulted from the <code>encode</code> or <code>setup</code> methods). Returns error level indicator value.</p>
</dd>
<dt>
<code>set_error_level(error_type, acceptable_error_level)</code>:</dt>
<dd>
<p> Set options that stop training when an acceptable error level has been reached (when a subsequent <code>encode</code> or <code>train_multiple</code> is performed). Parameters are:
</p>

<ul>
<li>
<p><code>error_type</code>: string, error type to display and use to stop training (must be 'MSE' or 'MAE').
</p>
</li>
<li>
<p><code>acceptable_error_level</code>: training stops when error is below this level.
</p>
</li>
</ul>
</dd>
<dt>
<code>mute(on)</code>:</dt>
<dd>
<p> Disable output of current error level when training (if parameter <code>on</code> is TRUE). </p>
</dd>
<dt>
<code>print()</code>:</dt>
<dd>
<p> Print NN structure. </p>
</dd>
<dt>
<code>show()</code>:</dt>
<dd>
<p> Print NN structure. </p>
</dd>
<dt>
<code>load(filename)</code>:</dt>
<dd>
<p> Retrieve the NN from specified file. </p>
</dd>
<dt>
<code>save(filename)</code>:</dt>
<dd>
<p> Save the NN to specified file. </p>
</dd>
</dl>
<p>The following methods are inherited (from the corresponding class):
objectPointer ("RcppClass"), initialize ("RcppClass"), show ("RcppClass")
</p>


<h3>Note</h3>

<p>This R module maintains an internal Back-Propagation (BP) multilayer perceptron NN (described in Simpson (1991) as the vanilla back-propagation algorithm), which can be used to store input-output vector pairs. Since the nodes (PEs) in computing layers of this BP implementation apply the logistic sigmoid threshold function, their output is in [0 1] range (and so should the desired output vector values).
</p>
<p>(This object uses Rcpp to employ 'bp_nn' class in nnlib2.)</p>


<h3>Author(s)</h3>

<p>Vasilis N. Nikolaidis &lt;vnnikolaidis@gmail.com&gt;
</p>


<h3>References</h3>

<p>Simpson, P. K. (1991). Artificial neural systems: Foundations, paradigms, applications, and implementations. New York: Pergamon Press.
</p>


<h3>See Also</h3>

<p><code>Autoencoder</code>.</p>


<h3>Examples</h3>

<pre><code class="language-R"># create some data...
iris_s                  &lt;- as.matrix(scale(iris[1:4]))

# use a randomly picked subset of (scaled) iris data for training.
training_cases          &lt;- sample(1:nrow(iris_s), nrow(iris_s)/2,replace=FALSE)
train_set               &lt;- iris_s[training_cases,]
train_class_ids         &lt;- as.integer(iris$Species[training_cases])
train_num_cases         &lt;- nrow(train_set)
train_num_variables     &lt;- ncol(train_set)
train_num_classes       &lt;- max(train_class_ids)

# create output dataset to be used for training.
# Here we encode class as 0s and 1s (one-hot encoding).

train_set_data_out &lt;- matrix(
          data = 0,
          nrow = train_num_cases,
          ncol = train_num_classes)

# now for each case, assign a 1 to the column corresponding to its class, 0 otherwise
# (note: there are better R ways to do this in R)
for(r in 1:train_num_cases) train_set_data_out[r,train_class_ids[r]]=1

# done with data, let's use BP...
bp&lt;-new("BP")

bp$encode(train_set,train_set_data_out,0.8,10000,2,4)

# let's test by recalling the original training set...
bp_output &lt;- bp$recall(train_set)

cat("- Using this demo's encoding, recalled class is:\n")
print(apply(bp_output,1,which.max))
cat("- BP success in recalling correct class is: ",
  sum(apply(bp_output,1,which.max)==train_class_ids)," out of ",
  train_num_cases, "\n")

# Let's see how well it recalls the entire Iris set:
bp_output &lt;- bp$recall(iris_s)

# show output
cat("\n- Recalling entire Iris set returns:\n")
print(bp_output)
cat("- Using this demo's encoding, original class is:\n")
print(as.integer(iris$Species))
cat("- Using this demo's encoding, recalled class is:\n")
bp_classification &lt;- apply(bp_output,1,which.max)
print(bp_classification)
cat("- BP success in recalling correct class is: ",
  sum(apply(bp_output,1,which.max)==as.integer(iris$Species)),
  "out of ", nrow(iris_s), "\n")
plot(iris_s, pch=bp_classification, main="Iris classified by a partialy trained BP (module)")
</code></pre>


</div>