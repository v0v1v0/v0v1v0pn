<div class="container">

<table style="width: 100%;"><tr>
<td>db_compute</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>S3 implementation of <code>db_compute</code> for Athena</h2>

<h3>Description</h3>

<p>This is a backend function for dplyr's <code>compute</code> function. Users won't be required to access and run this function.
</p>


<h3>Usage</h3>

<pre><code class="language-R">db_compute.AthenaConnection(
  con,
  table,
  sql,
  ...,
  overwrite = FALSE,
  temporary = FALSE,
  unique_indexes = list(),
  indexes = list(),
  analyze = TRUE,
  in_transaction = FALSE,
  partition = NULL,
  s3_location = NULL,
  file_type = c("csv", "tsv", "parquet"),
  compress = FALSE
)

sql_query_save.AthenaConnection(con, sql, name, temporary = TRUE, with, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>con</code></td>
<td>
<p>A <code>dbConnect</code> object, as returned by <code>dbConnect()</code></p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>table</code></td>
<td>
<p>Table name, if left default noctua will use the default from <code>dplyr</code>'s <code>compute</code> function.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>sql</code></td>
<td>
<p>SQL code to be sent to the data</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>...</code></td>
<td>
<p>passes <code>noctua</code> table creation parameters: [<code>file_type</code>,<code>s3_location</code>,<code>partition</code>]</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>overwrite</code></td>
<td>
<p>Allows overwriting the destination table. Cannot be <code>TRUE</code> if <code>append</code> is also <code>TRUE</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>temporary</code></td>
<td>
<p>if TRUE, will create a temporary table that is local to this connection and will be automatically deleted when the connection expires</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>unique_indexes</code></td>
<td>
<p>a list of character vectors. Each element of the list will create a new unique index over the specified column(s). Duplicate rows will result in failure.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>indexes</code></td>
<td>
<p>a list of character vectors. Each element of the list will create a new index.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>analyze</code></td>
<td>
<p>if TRUE (the default), will automatically ANALYZE the new table so that the query optimiser has useful information.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>in_transaction</code></td>
<td>
<p>Should the table creation be wrapped in a transaction? This typically makes things faster, but you may want to suppress if the database doesn't support transactions, or you're wrapping in a transaction higher up (and your database doesn't support nested transactions.)</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>partition</code></td>
<td>
<p>Partition Athena table (needs to be a named list or vector) for example: <code>c(var1 = "2019-20-13")</code></p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>s3_location</code></td>
<td>
<p>s3 bucket to store Athena table, must be set as a s3 uri for example ("s3://mybucket/data/")</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>file_type</code></td>
<td>
<p>What file type to store data.frame on s3, noctua currently supports ["tsv", "csv", "parquet"]. Default delimited file type is "tsv", in previous versions
of <code>noctua (=&lt; 1.4.0)</code> file type "csv" was used as default. The reason for the change is that columns containing <code>Array/JSON</code> format cannot be written to
Athena due to the separating value ",". This would cause issues with AWS Athena.
<strong>Note:</strong> "parquet" format is supported by the <code>arrow</code> package and it will need to be installed to utilise the "parquet" format.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>compress</code></td>
<td>
<p><code>FALSE | TRUE</code> To determine if to compress file.type. If file type is ["csv", "tsv"] then "gzip" compression is used, for file type "parquet"
"snappy" compression is used.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>name</code></td>
<td>
<p>Table name, if left default noctua will use the default from <code>dplyr</code>'s <code>compute</code> function.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>with</code></td>
<td>
<p>An optional WITH clause for the CREATE TABLE statement.
</p>

<ul>
<li>
<p><code>file_type:</code> What file type to store data.frame on s3, noctua currently supports ["NULL","csv", "parquet", "json"].
<code>"NULL"</code> will let Athena set the file_type for you.
</p>
</li>
<li>
<p><code>s3_location:</code> s3 bucket to store Athena table, must be set as a s3 uri for example ("s3://mybucket/data/")
</p>
</li>
<li>
<p><code>partition:</code> Partition Athena table, requires to be a partitioned variable from previous table.</p>
</li>
</ul>
</td>
</tr>
</table>
<h3>Value</h3>

<p><code>db_compute</code> returns table name
</p>


<h3>See Also</h3>

<p><code>AthenaWriteTables</code> <code>backend_dbplyr_v2</code> <code>backend_dbplyr_v1</code>
</p>


<h3>Examples</h3>

<pre><code class="language-R">## Not run: 
# Note:
# - Require AWS Account to run below example.
# - Different connection methods can be used please see `noctua::dbConnect` documentation

library(DBI)
library(dplyr)

# Demo connection to Athena using profile name
con &lt;- dbConnect(noctua::athena())

# Write data.frame to Athena table
copy_to(con, mtcars,
  s3_location = "s3://mybucket/data/"
)

# Write Athena table from tbl_sql
athena_mtcars &lt;- tbl(con, "mtcars")
mtcars_filter &lt;- athena_mtcars %&gt;% filter(gear &gt;= 4)

# create athena with unique table name
mtcars_filer %&gt;%
  compute()

# create athena with specified name and s3 location
mtcars_filer %&gt;%
  compute("mtcars_filer",
    s3_location = "s3://mybucket/mtcars_filer/"
  )

# Disconnect from Athena
dbDisconnect(con)

## End(Not run)
</code></pre>


</div>