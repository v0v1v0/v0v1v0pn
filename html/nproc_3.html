<div class="container">

<table style="width: 100%;"><tr>
<td>npc</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Construct a Neyman-Pearson Classifier from a sample of class 0 and class 1.</h2>

<h3>Description</h3>

<p>Given a type I error upper bound alpha and a violation upper bound delta, <code>npc</code> calculates the Neyman-Pearson Classifier
which controls the type I error under alpha with probability at least 1-delta.
</p>


<h3>Usage</h3>

<pre><code class="language-R">npc(x = NULL, y, method = c("logistic", "penlog", "svm", "randomforest",
  "lda", "slda", "nb", "nnb", "ada", "tree"), alpha = 0.05, delta = 0.05,
  split = 1, split.ratio = 0.5, n.cores = 1, band = FALSE,
  nfolds = 10, randSeed = 0, warning = TRUE, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>x</code></td>
<td>
<p>n * p observation matrix. n observations, p covariates.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>y</code></td>
<td>
<p>n 0/1 observatons.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>method</code></td>
<td>
<p>base classification method.
</p>

<ul>
<li>
<p> logistic: Logistic regression. glm function with family = 'binomial'
</p>
</li>
<li>
<p> penlog: Penalized logistic regression with LASSO penalty. <code>glmnet</code> in <code>glmnet</code> package
</p>
</li>
<li>
<p> svm: Support Vector Machines. <code>svm</code> in <code>e1071</code> package
</p>
</li>
<li>
<p> randomforest: Random Forest. <code>randomForest</code> in <code>randomForest</code> package
</p>
</li>
<li>
<p> lda: Linear Discriminant Analysis. <code>lda</code> in <code>MASS</code> package
</p>
</li>
<li>
<p> slda: Sparse Linear Discriminant Analysis with LASSO penalty.
</p>
</li>
<li>
<p> nb: Naive Bayes. <code>naiveBayes</code> in <code>e1071</code> package
</p>
</li>
<li>
<p> nnb: Nonparametric Naive Bayes. <code>naive_bayes</code> in <code>naivebayes</code> package
</p>
</li>
<li>
<p> ada: Ada-Boost. <code>ada</code> in <code>ada</code> package
</p>
</li>
</ul>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>alpha</code></td>
<td>
<p>the desirable upper bound on type I error. Default = 0.05.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>delta</code></td>
<td>
<p>the violation rate of the type I error. Default = 0.05.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>split</code></td>
<td>
<p>the number of splits for the class 0 sample. Default = 1. For ensemble
version, choose split &gt; 1.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>split.ratio</code></td>
<td>
<p>the ratio of splits used for the class 0 sample to train the
base classifier. The rest are used to estimate the threshold. Can also be set to be "adaptive", which will be determined using a data-driven method implemented in <code>find.optim.split</code>. Default = 0.5.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>n.cores</code></td>
<td>
<p>number of cores used for parallel computing. Default = 1. WARNING:
windows machine is not supported.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>band</code></td>
<td>
<p>whether to generate both lower and upper bounds of type II error. Default = FALSE.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>nfolds</code></td>
<td>
<p>number of folds for performing adaptive split ratio selection. Default = 10.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>randSeed</code></td>
<td>
<p>the random seed used in the algorithm.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>warning</code></td>
<td>
<p>whether to show various warnings in the program. Default = TRUE.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>...</code></td>
<td>
<p>additional arguments.</p>
</td>
</tr>
</table>
<h3>Value</h3>

<p>An object with S3 class npc.
</p>
<table>
<tr style="vertical-align: top;">
<td><code>fits</code></td>
<td>
<p>a list of length max(1,split), represents the fit during each split.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>method</code></td>
<td>
<p>the base classification method.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>split</code></td>
<td>
<p>the number of splits used.</p>
</td>
</tr>
</table>
<h3>References</h3>

<p>Xin Tong, Yang Feng, and Jingyi Jessica Li (2018), Neyman-Pearson (NP) classification algorithms and NP receiver operating characteristic (NP-ROC), <em>Science Advances</em>, <b>4</b>, 2, eaao1659.
</p>


<h3>See Also</h3>

<p><code>nproc</code> and <code>predict.npc</code>
</p>


<h3>Examples</h3>

<pre><code class="language-R">set.seed(1)
n = 1000
x = matrix(rnorm(n*2),n,2)
c = 1+3*x[,1]
y = rbinom(n,1,1/(1+exp(-c)))
xtest = matrix(rnorm(n*2),n,2)
ctest = 1+3*xtest[,1]
ytest = rbinom(n,1,1/(1+exp(-ctest)))

##Use lda classifier and the default type I error control with alpha=0.05, delta=0.05
fit = npc(x, y, method = 'lda')
pred = predict(fit,xtest)
fit.score = predict(fit,x)
accuracy = mean(pred$pred.label==ytest)
cat('Overall Accuracy: ',  accuracy,'\n')
ind0 = which(ytest==0)
typeI = mean(pred$pred.label[ind0]!=ytest[ind0]) #type I error on test set
cat('Type I error: ', typeI, '\n')

## Not run: 
##Ensembled lda classifier with split = 11,  alpha=0.05, delta=0.05
fit = npc(x, y, method = 'lda', split = 11)
pred = predict(fit,xtest)
accuracy = mean(pred$pred.label==ytest)
cat('Overall Accuracy: ',  accuracy,'\n')
ind0 = which(ytest==0)
typeI = mean(pred$pred.label[ind0]!=ytest[ind0]) #type I error on test set
cat('Type I error: ', typeI, '\n')

##Now, change the method to logistic regression and change alpha to 0.1
fit = npc(x, y, method = 'logistic', alpha = 0.1)
pred = predict(fit,xtest)
accuracy = mean(pred$pred.label==ytest)
cat('Overall Accuracy: ',  accuracy,'\n')
ind0 = which(ytest==0)
typeI = mean(pred$pred.label[ind0]!=ytest[ind0]) #type I error on test set
cat('Type I error: ', typeI, '\n')

##Now, change the method to adaboost
fit = npc(x, y, method = 'ada', alpha = 0.1)
pred = predict(fit,xtest)
accuracy = mean(pred$pred.label==ytest)
cat('Overall Accuracy: ',  accuracy,'\n')
ind0 = which(ytest==0)
typeI = mean(pred$pred.label[ind0]!=ytest[ind0]) #type I error on test set
cat('Type I error: ', typeI, '\n')

##Now, try the adaptive splitting ratio
fit = npc(x, y, method = 'ada', alpha = 0.1, split.ratio = 'adaptive')
pred = predict(fit,xtest)
accuracy = mean(pred$pred.label==ytest)
cat('Overall Accuracy: ',  accuracy,'\n')
ind0 = which(ytest==0)
typeI = mean(pred$pred.label[ind0]!=ytest[ind0]) #type I error on test set
cat('Type I error: ', typeI, '\n')
cat('Splitting ratio:', fit$split.ratio)

## End(Not run)
</code></pre>


</div>